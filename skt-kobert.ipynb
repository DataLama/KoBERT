{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet import gluon, autograd\n",
    "import gluonnlp as nlp\n",
    "from mxnet import nd \n",
    "import mxnet as mx\n",
    "import time\n",
    "import itertools\n",
    "import random\n",
    "import csv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from kobert.mxnet_kobert import get_mxnet_kobert_model\n",
    "from kobert.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading KoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "devices = [mx.gpu(i) for i in range(8)]\n",
    "bert_base, vocab = get_mxnet_kobert_model(use_decoder=False, use_classifier=False, ctx=devices)\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset\n",
    "dataset = pd.read_csv('data/voc-web-train.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## label_dict\n",
    "utils_config = dict()\n",
    "label = sorted(dataset['class'].unique())\n",
    "utils_config['label2idx'] = {c:i for i, c in enumerate(label)}\n",
    "utils_config['idx2label'] = {i:c for i, c in enumerate(label)}\n",
    "with open('config.json','w') as f:\n",
    "    json.dump(utils_config,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#### for train test\n",
    "dataset.drop(['class_1','class_2'],axis=1, inplace=True) # drop column\n",
    "dataset = dataset.loc[dataset['text'].isna().apply(lambda elm: not elm), :]\n",
    "train, validation = train_test_split(dataset, test_size=0.1, random_state=777)\n",
    "train['class'] = train['class'].apply(lambda x:utils_config['label2idx'][x])\n",
    "validation['class'] = validation['class'].apply(lambda x:utils_config['label2idx'][x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('train.npy',train.to_numpy()[:,1:])\n",
    "np.save('validation.npy',validation.to_numpy()[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = nlp.data.NumpyDataset('train.npy',allow_pickle=True)\n",
    "dataset_validation = nlp.data.NumpyDataset('validation.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(mx.gluon.data.Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "        sent_dataset = gluon.data.SimpleDataset([[\n",
    "            i[sent_idx],\n",
    "        ] for i in dataset])\n",
    "        self.sentences = sent_dataset.transform(transform)\n",
    "        self.labels = gluon.data.SimpleDataset(\n",
    "            [np.array(np.int32(i[label_idx])) for i in dataset])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_validation = BERTDataset(dataset_validation, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Bert fine-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Block):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 num_classes=2,\n",
    "                 dropout=None,\n",
    "                 prefix=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__(prefix=prefix, params=params)\n",
    "        self.bert = bert\n",
    "        with self.name_scope():\n",
    "            self.classifier = nn.HybridSequential(prefix=prefix)\n",
    "            if dropout:\n",
    "                self.classifier.add(nn.Dropout(rate=dropout))\n",
    "            self.classifier.add(nn.Dense(units=num_classes))\n",
    "\n",
    "    def forward(self, inputs, token_types, valid_length=None):\n",
    "        _, pooler = self.bert(inputs, token_types, valid_length)\n",
    "        return self.classifier(pooler)\n",
    "                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bert_base, num_classes=63, dropout=0.1)\n",
    "# 분류 레이어만 초기화 한다. \n",
    "model.classifier.initialize(init=mx.init.Normal(0.02), ctx=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.hybridize()\n",
    "\n",
    "# softmax cross entropy loss for classification\n",
    "loss_function = gluon.loss.SoftmaxCELoss()\n",
    "\n",
    "metric = mx.metric.TopKAccuracy(top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "lr = 5e-5\n",
    "\n",
    "train_dataloader = mx.gluon.data.DataLoader(data_train, batch_size=batch_size, num_workers=16)\n",
    "validation_dataloader = mx.gluon.data.DataLoader(data_validation, batch_size=int(batch_size/2), num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(), 'bertadam',{'learning_rate': lr, 'epsilon': 1e-9, 'wd':0.01}, update_on_kvstore=False)\n",
    "log_interval = 4\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LayerNorm과 Bias에는 Weight Decay를 적용하지 않는다. \n",
    "for _, v in model.collect_params('.*beta|.*gamma|.*bias').items():\n",
    "    v.wd_mult = 0.0\n",
    "params = [p for p in model.collect_params().values() if p.grad_req != 'null']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, data_iter, ctx):\n",
    "    acc = mx.metric.TopKAccuracy(top_k=3)\n",
    "    i = 0\n",
    "    for i, (t,v,s, label) in enumerate(data_iter):\n",
    "        token_ids = gluon.utils.split_and_load(t, ctx,even_split=False)\n",
    "        valid_length = gluon.utils.split_and_load(v, ctx,even_split=False)\n",
    "        segment_ids = gluon.utils.split_and_load(s, ctx,even_split=False)\n",
    "        label = gluon.utils.split_and_load(label, ctx,even_split=False)\n",
    "    \n",
    "        output = [model(ti, si, vl.astype('float32')) for ti, si, vl in zip(token_ids, segment_ids, valid_length)]\n",
    "        acc.update(preds=output, labels=label)\n",
    "        if i > 1000:\n",
    "            break\n",
    "        i += 1\n",
    "    return(acc.get()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate warmup을 위한 준비 \n",
    "accumulate = 4\n",
    "step_size = batch_size * accumulate if accumulate else batch_size\n",
    "num_train_examples = len(data_train)\n",
    "num_train_steps = int(num_train_examples / step_size * num_epochs)\n",
    "warmup_ratio = 0.1\n",
    "num_warmup_steps = int(num_train_steps * warmup_ratio)\n",
    "step_num = 0\n",
    "all_model_params = model.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set grad_req if gradient accumulation is required\n",
    "if accumulate and accumulate > 1:\n",
    "    for p in params:\n",
    "        p.grad_req = 'add'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 50/2581] loss=52.2822, lr=0.0000004651, Top3-acc=0.034\n",
      "[Epoch 1 Batch 100/2581] loss=51.6737, lr=0.0000009302, Top3-acc=0.046\n",
      "[Epoch 1 Batch 150/2581] loss=50.4763, lr=0.0000014341, Top3-acc=0.083\n",
      "[Epoch 1 Batch 200/2581] loss=49.7722, lr=0.0000018992, Top3-acc=0.115\n",
      "[Epoch 1 Batch 250/2581] loss=48.8493, lr=0.0000024031, Top3-acc=0.142\n",
      "[Epoch 1 Batch 300/2581] loss=47.9204, lr=0.0000028682, Top3-acc=0.160\n",
      "[Epoch 1 Batch 350/2581] loss=47.0147, lr=0.0000033721, Top3-acc=0.178\n",
      "[Epoch 1 Batch 400/2581] loss=46.0174, lr=0.0000038372, Top3-acc=0.194\n",
      "[Epoch 1 Batch 450/2581] loss=44.8868, lr=0.0000043411, Top3-acc=0.211\n",
      "[Epoch 1 Batch 500/2581] loss=44.0108, lr=0.0000048062, Top3-acc=0.225\n",
      "[Epoch 1 Batch 550/2581] loss=42.7425, lr=0.0000053101, Top3-acc=0.241\n",
      "[Epoch 1 Batch 600/2581] loss=40.9505, lr=0.0000057752, Top3-acc=0.258\n",
      "[Epoch 1 Batch 650/2581] loss=39.1099, lr=0.0000062791, Top3-acc=0.278\n",
      "[Epoch 1 Batch 700/2581] loss=37.6074, lr=0.0000067442, Top3-acc=0.298\n",
      "[Epoch 1 Batch 750/2581] loss=36.8468, lr=0.0000072481, Top3-acc=0.316\n",
      "[Epoch 1 Batch 800/2581] loss=34.4653, lr=0.0000077132, Top3-acc=0.336\n",
      "[Epoch 1 Batch 850/2581] loss=32.7619, lr=0.0000082171, Top3-acc=0.355\n",
      "[Epoch 1 Batch 900/2581] loss=32.1976, lr=0.0000086822, Top3-acc=0.372\n",
      "[Epoch 1 Batch 950/2581] loss=31.7011, lr=0.0000091860, Top3-acc=0.388\n",
      "[Epoch 1 Batch 1000/2581] loss=30.7432, lr=0.0000096512, Top3-acc=0.403\n",
      "[Epoch 1 Batch 1050/2581] loss=29.0268, lr=0.0000101550, Top3-acc=0.418\n",
      "[Epoch 1 Batch 1100/2581] loss=27.6647, lr=0.0000106202, Top3-acc=0.433\n",
      "[Epoch 1 Batch 1150/2581] loss=28.2885, lr=0.0000111240, Top3-acc=0.445\n",
      "[Epoch 1 Batch 1200/2581] loss=27.2556, lr=0.0000115891, Top3-acc=0.457\n",
      "[Epoch 1 Batch 1250/2581] loss=26.7923, lr=0.0000120930, Top3-acc=0.468\n",
      "[Epoch 1 Batch 1300/2581] loss=25.3255, lr=0.0000125581, Top3-acc=0.479\n",
      "[Epoch 1 Batch 1350/2581] loss=25.3063, lr=0.0000130620, Top3-acc=0.489\n",
      "[Epoch 1 Batch 1400/2581] loss=24.0067, lr=0.0000135271, Top3-acc=0.499\n",
      "[Epoch 1 Batch 1450/2581] loss=24.2000, lr=0.0000140310, Top3-acc=0.508\n",
      "[Epoch 1 Batch 1500/2581] loss=22.4604, lr=0.0000144961, Top3-acc=0.518\n",
      "[Epoch 1 Batch 1550/2581] loss=22.0974, lr=0.0000150000, Top3-acc=0.527\n",
      "[Epoch 1 Batch 1600/2581] loss=21.6518, lr=0.0000154651, Top3-acc=0.536\n",
      "[Epoch 1 Batch 1650/2581] loss=21.2328, lr=0.0000159690, Top3-acc=0.544\n",
      "[Epoch 1 Batch 1700/2581] loss=21.4718, lr=0.0000164341, Top3-acc=0.551\n",
      "[Epoch 1 Batch 1750/2581] loss=19.8683, lr=0.0000169380, Top3-acc=0.559\n",
      "[Epoch 1 Batch 1800/2581] loss=19.5732, lr=0.0000174031, Top3-acc=0.566\n",
      "[Epoch 1 Batch 1850/2581] loss=18.7278, lr=0.0000179070, Top3-acc=0.573\n",
      "[Epoch 1 Batch 1900/2581] loss=19.0251, lr=0.0000183721, Top3-acc=0.580\n",
      "[Epoch 1 Batch 1950/2581] loss=18.1388, lr=0.0000188760, Top3-acc=0.586\n",
      "[Epoch 1 Batch 2000/2581] loss=18.7902, lr=0.0000193411, Top3-acc=0.592\n",
      "[Epoch 1 Batch 2050/2581] loss=18.3190, lr=0.0000198450, Top3-acc=0.598\n",
      "[Epoch 1 Batch 2100/2581] loss=17.3547, lr=0.0000203101, Top3-acc=0.604\n",
      "[Epoch 1 Batch 2150/2581] loss=17.9708, lr=0.0000208140, Top3-acc=0.609\n",
      "[Epoch 1 Batch 2200/2581] loss=16.7684, lr=0.0000212791, Top3-acc=0.615\n",
      "[Epoch 1 Batch 2250/2581] loss=17.1978, lr=0.0000217829, Top3-acc=0.620\n",
      "[Epoch 1 Batch 2300/2581] loss=17.4300, lr=0.0000222481, Top3-acc=0.625\n",
      "[Epoch 1 Batch 2350/2581] loss=16.8091, lr=0.0000227519, Top3-acc=0.629\n",
      "[Epoch 1 Batch 2400/2581] loss=16.4967, lr=0.0000232171, Top3-acc=0.634\n",
      "[Epoch 1 Batch 2450/2581] loss=16.3704, lr=0.0000237209, Top3-acc=0.638\n",
      "[Epoch 1 Batch 2500/2581] loss=16.5426, lr=0.0000241860, Top3-acc=0.642\n",
      "[Epoch 1 Batch 2550/2581] loss=15.5751, lr=0.0000246899, Top3-acc=0.647\n",
      "validation Top3-Acc : 0.8622643565435327\n",
      "[Epoch 2 Batch 50/2581] loss=16.0058, lr=0.0000254651, Top3-acc=0.857\n",
      "[Epoch 2 Batch 100/2581] loss=15.7282, lr=0.0000259302, Top3-acc=0.861\n",
      "[Epoch 2 Batch 150/2581] loss=15.5666, lr=0.0000264341, Top3-acc=0.863\n",
      "[Epoch 2 Batch 200/2581] loss=15.8787, lr=0.0000268992, Top3-acc=0.861\n",
      "[Epoch 2 Batch 250/2581] loss=14.6060, lr=0.0000274031, Top3-acc=0.863\n",
      "[Epoch 2 Batch 300/2581] loss=14.7223, lr=0.0000278682, Top3-acc=0.865\n",
      "[Epoch 2 Batch 350/2581] loss=14.7300, lr=0.0000283721, Top3-acc=0.867\n",
      "[Epoch 2 Batch 400/2581] loss=15.0319, lr=0.0000288372, Top3-acc=0.867\n",
      "[Epoch 2 Batch 450/2581] loss=13.8604, lr=0.0000293411, Top3-acc=0.869\n",
      "[Epoch 2 Batch 500/2581] loss=14.3231, lr=0.0000298062, Top3-acc=0.871\n",
      "[Epoch 2 Batch 550/2581] loss=13.9185, lr=0.0000303101, Top3-acc=0.872\n",
      "[Epoch 2 Batch 600/2581] loss=13.6077, lr=0.0000307752, Top3-acc=0.873\n",
      "[Epoch 2 Batch 650/2581] loss=14.0252, lr=0.0000312791, Top3-acc=0.874\n",
      "[Epoch 2 Batch 700/2581] loss=14.5044, lr=0.0000317442, Top3-acc=0.875\n",
      "[Epoch 2 Batch 750/2581] loss=13.6347, lr=0.0000322481, Top3-acc=0.875\n",
      "[Epoch 2 Batch 800/2581] loss=13.3722, lr=0.0000327132, Top3-acc=0.876\n",
      "[Epoch 2 Batch 850/2581] loss=13.5621, lr=0.0000332171, Top3-acc=0.877\n",
      "[Epoch 2 Batch 900/2581] loss=13.3812, lr=0.0000336822, Top3-acc=0.878\n",
      "[Epoch 2 Batch 950/2581] loss=13.0129, lr=0.0000341860, Top3-acc=0.879\n",
      "[Epoch 2 Batch 1000/2581] loss=13.4336, lr=0.0000346512, Top3-acc=0.880\n",
      "[Epoch 2 Batch 1050/2581] loss=12.5248, lr=0.0000351550, Top3-acc=0.881\n",
      "[Epoch 2 Batch 1100/2581] loss=12.6265, lr=0.0000356202, Top3-acc=0.882\n",
      "[Epoch 2 Batch 1150/2581] loss=13.3014, lr=0.0000361240, Top3-acc=0.882\n",
      "[Epoch 2 Batch 1200/2581] loss=13.0658, lr=0.0000365891, Top3-acc=0.883\n",
      "[Epoch 2 Batch 1250/2581] loss=12.8899, lr=0.0000370930, Top3-acc=0.883\n",
      "[Epoch 2 Batch 1300/2581] loss=12.6620, lr=0.0000375581, Top3-acc=0.884\n",
      "[Epoch 2 Batch 1350/2581] loss=12.8796, lr=0.0000380620, Top3-acc=0.884\n",
      "[Epoch 2 Batch 1400/2581] loss=11.6898, lr=0.0000385271, Top3-acc=0.885\n",
      "[Epoch 2 Batch 1450/2581] loss=12.9423, lr=0.0000390310, Top3-acc=0.885\n",
      "[Epoch 2 Batch 1500/2581] loss=11.4272, lr=0.0000394961, Top3-acc=0.886\n",
      "[Epoch 2 Batch 1550/2581] loss=11.9797, lr=0.0000400000, Top3-acc=0.887\n",
      "[Epoch 2 Batch 1600/2581] loss=12.6060, lr=0.0000404651, Top3-acc=0.887\n",
      "[Epoch 2 Batch 1650/2581] loss=12.3939, lr=0.0000409690, Top3-acc=0.888\n",
      "[Epoch 2 Batch 1700/2581] loss=12.8614, lr=0.0000414341, Top3-acc=0.888\n",
      "[Epoch 2 Batch 1750/2581] loss=11.9218, lr=0.0000419380, Top3-acc=0.888\n",
      "[Epoch 2 Batch 1800/2581] loss=12.1267, lr=0.0000424031, Top3-acc=0.889\n",
      "[Epoch 2 Batch 1850/2581] loss=11.2850, lr=0.0000429070, Top3-acc=0.889\n",
      "[Epoch 2 Batch 1900/2581] loss=11.7240, lr=0.0000433721, Top3-acc=0.890\n",
      "[Epoch 2 Batch 1950/2581] loss=10.8665, lr=0.0000438760, Top3-acc=0.891\n",
      "[Epoch 2 Batch 2000/2581] loss=11.8924, lr=0.0000443411, Top3-acc=0.891\n",
      "[Epoch 2 Batch 2050/2581] loss=11.6775, lr=0.0000448450, Top3-acc=0.892\n",
      "[Epoch 2 Batch 2100/2581] loss=11.2924, lr=0.0000453101, Top3-acc=0.892\n",
      "[Epoch 2 Batch 2150/2581] loss=12.1750, lr=0.0000458140, Top3-acc=0.892\n",
      "[Epoch 2 Batch 2200/2581] loss=10.7798, lr=0.0000462791, Top3-acc=0.893\n",
      "[Epoch 2 Batch 2250/2581] loss=11.1646, lr=0.0000467829, Top3-acc=0.893\n",
      "[Epoch 2 Batch 2300/2581] loss=11.2754, lr=0.0000472481, Top3-acc=0.894\n",
      "[Epoch 2 Batch 2350/2581] loss=10.9627, lr=0.0000477519, Top3-acc=0.894\n",
      "[Epoch 2 Batch 2400/2581] loss=10.7646, lr=0.0000482171, Top3-acc=0.895\n",
      "[Epoch 2 Batch 2450/2581] loss=10.7466, lr=0.0000487209, Top3-acc=0.895\n",
      "[Epoch 2 Batch 2500/2581] loss=11.4803, lr=0.0000491860, Top3-acc=0.896\n",
      "[Epoch 2 Batch 2550/2581] loss=10.7624, lr=0.0000496899, Top3-acc=0.896\n",
      "validation Top3-Acc : 0.9093385638008064\n",
      "[Epoch 3 Batch 50/2581] loss=11.4788, lr=0.0000499483, Top3-acc=0.911\n",
      "[Epoch 3 Batch 100/2581] loss=10.7532, lr=0.0000498967, Top3-acc=0.915\n",
      "[Epoch 3 Batch 150/2581] loss=11.3268, lr=0.0000498407, Top3-acc=0.913\n",
      "[Epoch 3 Batch 200/2581] loss=11.5094, lr=0.0000497891, Top3-acc=0.912\n",
      "[Epoch 3 Batch 250/2581] loss=10.3498, lr=0.0000497331, Top3-acc=0.914\n",
      "[Epoch 3 Batch 300/2581] loss=10.7028, lr=0.0000496814, Top3-acc=0.915\n",
      "[Epoch 3 Batch 350/2581] loss=10.5968, lr=0.0000496255, Top3-acc=0.915\n",
      "[Epoch 3 Batch 400/2581] loss=11.0522, lr=0.0000495738, Top3-acc=0.915\n",
      "[Epoch 3 Batch 450/2581] loss=10.1910, lr=0.0000495179, Top3-acc=0.916\n",
      "[Epoch 3 Batch 500/2581] loss=9.8747, lr=0.0000494662, Top3-acc=0.918\n",
      "[Epoch 3 Batch 550/2581] loss=9.9664, lr=0.0000494102, Top3-acc=0.918\n",
      "[Epoch 3 Batch 600/2581] loss=9.9218, lr=0.0000493586, Top3-acc=0.919\n",
      "[Epoch 3 Batch 650/2581] loss=10.6688, lr=0.0000493026, Top3-acc=0.919\n",
      "[Epoch 3 Batch 700/2581] loss=10.9602, lr=0.0000492510, Top3-acc=0.919\n",
      "[Epoch 3 Batch 750/2581] loss=9.9966, lr=0.0000491950, Top3-acc=0.920\n",
      "[Epoch 3 Batch 800/2581] loss=10.0073, lr=0.0000491433, Top3-acc=0.921\n",
      "[Epoch 3 Batch 850/2581] loss=10.1964, lr=0.0000490874, Top3-acc=0.921\n",
      "[Epoch 3 Batch 900/2581] loss=9.8118, lr=0.0000490357, Top3-acc=0.922\n",
      "[Epoch 3 Batch 950/2581] loss=9.5927, lr=0.0000489798, Top3-acc=0.923\n",
      "[Epoch 3 Batch 1000/2581] loss=9.6735, lr=0.0000489281, Top3-acc=0.923\n",
      "[Epoch 3 Batch 1050/2581] loss=9.1571, lr=0.0000488721, Top3-acc=0.924\n",
      "[Epoch 3 Batch 1100/2581] loss=9.6174, lr=0.0000488205, Top3-acc=0.924\n",
      "[Epoch 3 Batch 1150/2581] loss=10.0072, lr=0.0000487645, Top3-acc=0.924\n",
      "[Epoch 3 Batch 1200/2581] loss=9.7026, lr=0.0000487129, Top3-acc=0.924\n",
      "[Epoch 3 Batch 1250/2581] loss=9.9399, lr=0.0000486569, Top3-acc=0.924\n",
      "[Epoch 3 Batch 1300/2581] loss=9.6560, lr=0.0000486053, Top3-acc=0.925\n",
      "[Epoch 3 Batch 1350/2581] loss=9.8548, lr=0.0000485493, Top3-acc=0.925\n",
      "[Epoch 3 Batch 1400/2581] loss=8.7552, lr=0.0000484976, Top3-acc=0.926\n",
      "[Epoch 3 Batch 1450/2581] loss=9.5675, lr=0.0000484417, Top3-acc=0.926\n",
      "[Epoch 3 Batch 1500/2581] loss=8.7452, lr=0.0000483900, Top3-acc=0.927\n",
      "[Epoch 3 Batch 1550/2581] loss=9.4802, lr=0.0000483341, Top3-acc=0.927\n",
      "[Epoch 3 Batch 1600/2581] loss=9.8175, lr=0.0000482824, Top3-acc=0.927\n",
      "[Epoch 3 Batch 1650/2581] loss=9.5124, lr=0.0000482264, Top3-acc=0.927\n",
      "[Epoch 3 Batch 1700/2581] loss=9.8597, lr=0.0000481748, Top3-acc=0.927\n",
      "[Epoch 3 Batch 1750/2581] loss=8.7881, lr=0.0000481188, Top3-acc=0.927\n",
      "[Epoch 3 Batch 1800/2581] loss=9.1967, lr=0.0000480672, Top3-acc=0.928\n",
      "[Epoch 3 Batch 1850/2581] loss=8.7339, lr=0.0000480112, Top3-acc=0.928\n",
      "[Epoch 3 Batch 1900/2581] loss=8.9705, lr=0.0000479595, Top3-acc=0.928\n",
      "[Epoch 3 Batch 1950/2581] loss=8.3158, lr=0.0000479036, Top3-acc=0.929\n",
      "[Epoch 3 Batch 2000/2581] loss=9.3817, lr=0.0000478519, Top3-acc=0.929\n",
      "[Epoch 3 Batch 2050/2581] loss=9.3124, lr=0.0000477960, Top3-acc=0.929\n",
      "[Epoch 3 Batch 2100/2581] loss=8.5584, lr=0.0000477443, Top3-acc=0.929\n",
      "[Epoch 3 Batch 2150/2581] loss=9.6831, lr=0.0000476883, Top3-acc=0.930\n",
      "[Epoch 3 Batch 2200/2581] loss=8.5980, lr=0.0000476367, Top3-acc=0.930\n",
      "[Epoch 3 Batch 2250/2581] loss=8.6315, lr=0.0000475807, Top3-acc=0.930\n",
      "[Epoch 3 Batch 2300/2581] loss=8.5923, lr=0.0000475291, Top3-acc=0.930\n",
      "[Epoch 3 Batch 2350/2581] loss=8.3678, lr=0.0000474731, Top3-acc=0.931\n",
      "[Epoch 3 Batch 2400/2581] loss=8.5404, lr=0.0000474214, Top3-acc=0.931\n",
      "[Epoch 3 Batch 2450/2581] loss=8.2123, lr=0.0000473655, Top3-acc=0.931\n",
      "[Epoch 3 Batch 2500/2581] loss=8.5705, lr=0.0000473138, Top3-acc=0.932\n",
      "[Epoch 3 Batch 2550/2581] loss=8.3402, lr=0.0000472579, Top3-acc=0.932\n",
      "validation Top3-Acc : 0.9115179252479023\n",
      "[Epoch 4 Batch 50/2581] loss=9.1439, lr=0.0000471718, Top3-acc=0.936\n",
      "[Epoch 4 Batch 100/2581] loss=8.4266, lr=0.0000471201, Top3-acc=0.938\n",
      "[Epoch 4 Batch 150/2581] loss=8.7351, lr=0.0000470641, Top3-acc=0.939\n",
      "[Epoch 4 Batch 200/2581] loss=8.9362, lr=0.0000470125, Top3-acc=0.937\n",
      "[Epoch 4 Batch 250/2581] loss=8.1287, lr=0.0000469565, Top3-acc=0.939\n",
      "[Epoch 4 Batch 300/2581] loss=8.5157, lr=0.0000469049, Top3-acc=0.940\n",
      "[Epoch 4 Batch 350/2581] loss=8.3741, lr=0.0000468489, Top3-acc=0.940\n",
      "[Epoch 4 Batch 400/2581] loss=8.8687, lr=0.0000467972, Top3-acc=0.941\n",
      "[Epoch 4 Batch 450/2581] loss=8.2477, lr=0.0000467413, Top3-acc=0.941\n",
      "[Epoch 4 Batch 500/2581] loss=7.8602, lr=0.0000466896, Top3-acc=0.942\n",
      "[Epoch 4 Batch 550/2581] loss=7.9756, lr=0.0000466337, Top3-acc=0.943\n",
      "[Epoch 4 Batch 600/2581] loss=7.9253, lr=0.0000465820, Top3-acc=0.944\n",
      "[Epoch 4 Batch 650/2581] loss=8.5067, lr=0.0000465260, Top3-acc=0.944\n",
      "[Epoch 4 Batch 700/2581] loss=8.4193, lr=0.0000464744, Top3-acc=0.944\n",
      "[Epoch 4 Batch 750/2581] loss=8.0068, lr=0.0000464184, Top3-acc=0.945\n",
      "[Epoch 4 Batch 800/2581] loss=7.8514, lr=0.0000463668, Top3-acc=0.945\n",
      "[Epoch 4 Batch 850/2581] loss=8.0832, lr=0.0000463108, Top3-acc=0.946\n",
      "[Epoch 4 Batch 900/2581] loss=7.6329, lr=0.0000462591, Top3-acc=0.947\n",
      "[Epoch 4 Batch 950/2581] loss=7.5795, lr=0.0000462032, Top3-acc=0.947\n",
      "[Epoch 4 Batch 1000/2581] loss=7.7360, lr=0.0000461515, Top3-acc=0.947\n",
      "[Epoch 4 Batch 1050/2581] loss=7.0869, lr=0.0000460956, Top3-acc=0.948\n",
      "[Epoch 4 Batch 1100/2581] loss=7.6756, lr=0.0000460439, Top3-acc=0.948\n",
      "[Epoch 4 Batch 1150/2581] loss=7.8200, lr=0.0000459879, Top3-acc=0.948\n",
      "[Epoch 4 Batch 1200/2581] loss=7.7114, lr=0.0000459363, Top3-acc=0.948\n",
      "[Epoch 4 Batch 1250/2581] loss=7.5130, lr=0.0000458803, Top3-acc=0.948\n",
      "[Epoch 4 Batch 1300/2581] loss=7.5539, lr=0.0000458287, Top3-acc=0.948\n",
      "[Epoch 4 Batch 1350/2581] loss=7.7780, lr=0.0000457727, Top3-acc=0.949\n",
      "[Epoch 4 Batch 1400/2581] loss=6.8175, lr=0.0000457211, Top3-acc=0.949\n",
      "[Epoch 4 Batch 1450/2581] loss=7.5726, lr=0.0000456651, Top3-acc=0.949\n",
      "[Epoch 4 Batch 1500/2581] loss=6.7138, lr=0.0000456134, Top3-acc=0.950\n",
      "[Epoch 4 Batch 1550/2581] loss=7.6412, lr=0.0000455575, Top3-acc=0.950\n",
      "[Epoch 4 Batch 1600/2581] loss=7.7148, lr=0.0000455058, Top3-acc=0.950\n",
      "[Epoch 4 Batch 1650/2581] loss=7.5337, lr=0.0000454498, Top3-acc=0.950\n",
      "[Epoch 4 Batch 1700/2581] loss=7.7240, lr=0.0000453982, Top3-acc=0.950\n",
      "[Epoch 4 Batch 1750/2581] loss=7.3258, lr=0.0000453422, Top3-acc=0.950\n",
      "[Epoch 4 Batch 1800/2581] loss=7.3792, lr=0.0000452906, Top3-acc=0.951\n",
      "[Epoch 4 Batch 1850/2581] loss=7.3631, lr=0.0000452346, Top3-acc=0.951\n",
      "[Epoch 4 Batch 1900/2581] loss=7.2335, lr=0.0000451830, Top3-acc=0.951\n",
      "[Epoch 4 Batch 1950/2581] loss=6.6401, lr=0.0000451270, Top3-acc=0.951\n",
      "[Epoch 4 Batch 2000/2581] loss=7.3958, lr=0.0000450753, Top3-acc=0.951\n",
      "[Epoch 4 Batch 2050/2581] loss=7.4172, lr=0.0000450194, Top3-acc=0.951\n",
      "[Epoch 4 Batch 2100/2581] loss=6.8143, lr=0.0000449677, Top3-acc=0.952\n",
      "[Epoch 4 Batch 2150/2581] loss=7.8191, lr=0.0000449118, Top3-acc=0.952\n",
      "[Epoch 4 Batch 2200/2581] loss=6.6285, lr=0.0000448601, Top3-acc=0.952\n",
      "[Epoch 4 Batch 2250/2581] loss=6.6878, lr=0.0000448041, Top3-acc=0.952\n",
      "[Epoch 4 Batch 2300/2581] loss=6.8071, lr=0.0000447525, Top3-acc=0.953\n",
      "[Epoch 4 Batch 2350/2581] loss=6.7646, lr=0.0000446965, Top3-acc=0.953\n",
      "[Epoch 4 Batch 2400/2581] loss=6.9259, lr=0.0000446449, Top3-acc=0.953\n",
      "[Epoch 4 Batch 2450/2581] loss=6.4856, lr=0.0000445889, Top3-acc=0.953\n",
      "[Epoch 4 Batch 2500/2581] loss=7.2421, lr=0.0000445372, Top3-acc=0.953\n",
      "[Epoch 4 Batch 2550/2581] loss=6.8532, lr=0.0000444813, Top3-acc=0.953\n",
      "validation Top3-Acc : 0.9212160836874795\n",
      "[Epoch 5 Batch 50/2581] loss=7.3481, lr=0.0000443952, Top3-acc=0.949\n",
      "[Epoch 5 Batch 100/2581] loss=6.7800, lr=0.0000443435, Top3-acc=0.955\n",
      "[Epoch 5 Batch 150/2581] loss=7.0861, lr=0.0000442876, Top3-acc=0.957\n",
      "[Epoch 5 Batch 200/2581] loss=7.3785, lr=0.0000442359, Top3-acc=0.957\n",
      "[Epoch 5 Batch 250/2581] loss=6.6219, lr=0.0000441799, Top3-acc=0.958\n",
      "[Epoch 5 Batch 300/2581] loss=6.9578, lr=0.0000441283, Top3-acc=0.958\n",
      "[Epoch 5 Batch 350/2581] loss=6.6509, lr=0.0000440723, Top3-acc=0.959\n",
      "[Epoch 5 Batch 400/2581] loss=7.3128, lr=0.0000440207, Top3-acc=0.959\n",
      "[Epoch 5 Batch 450/2581] loss=6.5341, lr=0.0000439647, Top3-acc=0.959\n",
      "[Epoch 5 Batch 500/2581] loss=6.4677, lr=0.0000439130, Top3-acc=0.960\n",
      "[Epoch 5 Batch 550/2581] loss=6.5346, lr=0.0000438571, Top3-acc=0.959\n",
      "[Epoch 5 Batch 600/2581] loss=6.2800, lr=0.0000438054, Top3-acc=0.960\n",
      "[Epoch 5 Batch 650/2581] loss=6.9737, lr=0.0000437495, Top3-acc=0.960\n",
      "[Epoch 5 Batch 700/2581] loss=6.9692, lr=0.0000436978, Top3-acc=0.961\n",
      "[Epoch 5 Batch 750/2581] loss=6.2536, lr=0.0000436418, Top3-acc=0.961\n",
      "[Epoch 5 Batch 800/2581] loss=6.4992, lr=0.0000435902, Top3-acc=0.961\n",
      "[Epoch 5 Batch 850/2581] loss=6.5423, lr=0.0000435342, Top3-acc=0.961\n",
      "[Epoch 5 Batch 900/2581] loss=5.9328, lr=0.0000434826, Top3-acc=0.962\n",
      "[Epoch 5 Batch 950/2581] loss=6.0877, lr=0.0000434266, Top3-acc=0.963\n",
      "[Epoch 5 Batch 1000/2581] loss=6.0024, lr=0.0000433749, Top3-acc=0.964\n",
      "[Epoch 5 Batch 1050/2581] loss=5.2913, lr=0.0000433190, Top3-acc=0.964\n",
      "[Epoch 5 Batch 1100/2581] loss=5.8589, lr=0.0000432673, Top3-acc=0.964\n",
      "[Epoch 5 Batch 1150/2581] loss=6.4671, lr=0.0000432114, Top3-acc=0.964\n",
      "[Epoch 5 Batch 1200/2581] loss=6.1797, lr=0.0000431597, Top3-acc=0.964\n",
      "[Epoch 5 Batch 1250/2581] loss=6.0498, lr=0.0000431037, Top3-acc=0.964\n",
      "[Epoch 5 Batch 1300/2581] loss=6.2918, lr=0.0000430521, Top3-acc=0.965\n",
      "[Epoch 5 Batch 1350/2581] loss=6.4166, lr=0.0000429961, Top3-acc=0.965\n",
      "[Epoch 5 Batch 1400/2581] loss=5.6397, lr=0.0000429445, Top3-acc=0.965\n",
      "[Epoch 5 Batch 1450/2581] loss=6.5242, lr=0.0000428885, Top3-acc=0.965\n",
      "[Epoch 5 Batch 1500/2581] loss=5.7509, lr=0.0000428368, Top3-acc=0.966\n",
      "[Epoch 5 Batch 1550/2581] loss=6.2156, lr=0.0000427809, Top3-acc=0.966\n",
      "[Epoch 5 Batch 1600/2581] loss=6.5401, lr=0.0000427292, Top3-acc=0.966\n",
      "[Epoch 5 Batch 1650/2581] loss=6.3113, lr=0.0000426733, Top3-acc=0.965\n",
      "[Epoch 5 Batch 1700/2581] loss=6.1594, lr=0.0000426216, Top3-acc=0.965\n",
      "[Epoch 5 Batch 1750/2581] loss=5.7911, lr=0.0000425656, Top3-acc=0.966\n",
      "[Epoch 5 Batch 1800/2581] loss=5.8119, lr=0.0000425140, Top3-acc=0.966\n",
      "[Epoch 5 Batch 1850/2581] loss=6.1784, lr=0.0000424580, Top3-acc=0.966\n",
      "[Epoch 5 Batch 1900/2581] loss=5.9102, lr=0.0000424064, Top3-acc=0.966\n",
      "[Epoch 5 Batch 1950/2581] loss=5.2632, lr=0.0000423504, Top3-acc=0.966\n",
      "[Epoch 5 Batch 2000/2581] loss=5.8854, lr=0.0000422988, Top3-acc=0.966\n",
      "[Epoch 5 Batch 2050/2581] loss=5.9137, lr=0.0000422428, Top3-acc=0.966\n",
      "[Epoch 5 Batch 2100/2581] loss=5.5416, lr=0.0000421911, Top3-acc=0.966\n",
      "[Epoch 5 Batch 2150/2581] loss=6.6235, lr=0.0000421352, Top3-acc=0.966\n",
      "[Epoch 5 Batch 2200/2581] loss=5.6108, lr=0.0000420835, Top3-acc=0.966\n",
      "[Epoch 5 Batch 2250/2581] loss=5.4928, lr=0.0000420276, Top3-acc=0.966\n",
      "[Epoch 5 Batch 2300/2581] loss=5.4861, lr=0.0000419759, Top3-acc=0.967\n",
      "[Epoch 5 Batch 2350/2581] loss=5.7895, lr=0.0000419199, Top3-acc=0.967\n",
      "[Epoch 5 Batch 2400/2581] loss=6.0939, lr=0.0000418683, Top3-acc=0.967\n",
      "[Epoch 5 Batch 2450/2581] loss=5.4519, lr=0.0000418123, Top3-acc=0.967\n",
      "[Epoch 5 Batch 2500/2581] loss=5.8905, lr=0.0000417607, Top3-acc=0.967\n",
      "[Epoch 5 Batch 2550/2581] loss=5.5921, lr=0.0000417047, Top3-acc=0.967\n",
      "validation Top3-Acc : 0.91554974392503\n",
      "[Epoch 6 Batch 50/2581] loss=5.9691, lr=0.0000416186, Top3-acc=0.966\n",
      "[Epoch 6 Batch 100/2581] loss=5.5861, lr=0.0000415669, Top3-acc=0.969\n",
      "[Epoch 6 Batch 150/2581] loss=6.0339, lr=0.0000415110, Top3-acc=0.969\n",
      "[Epoch 6 Batch 200/2581] loss=6.4418, lr=0.0000414593, Top3-acc=0.970\n",
      "[Epoch 6 Batch 250/2581] loss=5.7638, lr=0.0000414034, Top3-acc=0.969\n",
      "[Epoch 6 Batch 300/2581] loss=5.6444, lr=0.0000413517, Top3-acc=0.969\n",
      "[Epoch 6 Batch 350/2581] loss=5.4613, lr=0.0000412957, Top3-acc=0.970\n",
      "[Epoch 6 Batch 400/2581] loss=6.2246, lr=0.0000412441, Top3-acc=0.969\n",
      "[Epoch 6 Batch 450/2581] loss=5.6381, lr=0.0000411881, Top3-acc=0.970\n",
      "[Epoch 6 Batch 500/2581] loss=5.4511, lr=0.0000411365, Top3-acc=0.970\n",
      "[Epoch 6 Batch 550/2581] loss=5.4748, lr=0.0000410805, Top3-acc=0.971\n",
      "[Epoch 6 Batch 600/2581] loss=5.1302, lr=0.0000410288, Top3-acc=0.971\n",
      "[Epoch 6 Batch 650/2581] loss=5.6675, lr=0.0000409729, Top3-acc=0.972\n",
      "[Epoch 6 Batch 700/2581] loss=5.6867, lr=0.0000409212, Top3-acc=0.972\n",
      "[Epoch 6 Batch 750/2581] loss=5.5797, lr=0.0000408653, Top3-acc=0.972\n",
      "[Epoch 6 Batch 800/2581] loss=5.3439, lr=0.0000408136, Top3-acc=0.973\n",
      "[Epoch 6 Batch 850/2581] loss=5.4912, lr=0.0000407576, Top3-acc=0.973\n",
      "[Epoch 6 Batch 900/2581] loss=4.9246, lr=0.0000407060, Top3-acc=0.973\n",
      "[Epoch 6 Batch 950/2581] loss=5.1005, lr=0.0000406500, Top3-acc=0.974\n",
      "[Epoch 6 Batch 1000/2581] loss=4.9416, lr=0.0000405984, Top3-acc=0.974\n",
      "[Epoch 6 Batch 1050/2581] loss=4.4513, lr=0.0000405424, Top3-acc=0.974\n",
      "[Epoch 6 Batch 1100/2581] loss=5.1040, lr=0.0000404907, Top3-acc=0.974\n",
      "[Epoch 6 Batch 1150/2581] loss=5.0975, lr=0.0000404348, Top3-acc=0.974\n",
      "[Epoch 6 Batch 1200/2581] loss=5.1987, lr=0.0000403831, Top3-acc=0.974\n",
      "[Epoch 6 Batch 1250/2581] loss=5.0969, lr=0.0000403272, Top3-acc=0.974\n",
      "[Epoch 6 Batch 1300/2581] loss=5.3662, lr=0.0000402755, Top3-acc=0.974\n",
      "[Epoch 6 Batch 1350/2581] loss=5.5779, lr=0.0000402195, Top3-acc=0.974\n",
      "[Epoch 6 Batch 1400/2581] loss=4.8462, lr=0.0000401679, Top3-acc=0.975\n",
      "[Epoch 6 Batch 1450/2581] loss=5.4440, lr=0.0000401119, Top3-acc=0.975\n",
      "[Epoch 6 Batch 1500/2581] loss=4.9441, lr=0.0000400603, Top3-acc=0.975\n",
      "[Epoch 6 Batch 1550/2581] loss=5.3787, lr=0.0000400043, Top3-acc=0.975\n",
      "[Epoch 6 Batch 1600/2581] loss=5.6277, lr=0.0000399526, Top3-acc=0.974\n",
      "[Epoch 6 Batch 1650/2581] loss=5.2606, lr=0.0000398967, Top3-acc=0.974\n",
      "[Epoch 6 Batch 1700/2581] loss=5.3372, lr=0.0000398450, Top3-acc=0.974\n",
      "[Epoch 6 Batch 1750/2581] loss=4.5618, lr=0.0000397891, Top3-acc=0.974\n",
      "[Epoch 6 Batch 1800/2581] loss=5.2757, lr=0.0000397374, Top3-acc=0.975\n",
      "[Epoch 6 Batch 1850/2581] loss=5.1049, lr=0.0000396814, Top3-acc=0.975\n",
      "[Epoch 6 Batch 1900/2581] loss=4.7939, lr=0.0000396298, Top3-acc=0.975\n",
      "[Epoch 6 Batch 1950/2581] loss=4.2801, lr=0.0000395738, Top3-acc=0.975\n",
      "[Epoch 6 Batch 2000/2581] loss=4.8257, lr=0.0000395222, Top3-acc=0.975\n",
      "[Epoch 6 Batch 2050/2581] loss=4.9434, lr=0.0000394662, Top3-acc=0.975\n",
      "[Epoch 6 Batch 2100/2581] loss=4.2871, lr=0.0000394146, Top3-acc=0.975\n",
      "[Epoch 6 Batch 2150/2581] loss=5.4130, lr=0.0000393586, Top3-acc=0.975\n",
      "[Epoch 6 Batch 2200/2581] loss=4.3808, lr=0.0000393069, Top3-acc=0.976\n",
      "[Epoch 6 Batch 2250/2581] loss=4.4257, lr=0.0000392510, Top3-acc=0.976\n",
      "[Epoch 6 Batch 2300/2581] loss=4.5438, lr=0.0000391993, Top3-acc=0.976\n",
      "[Epoch 6 Batch 2350/2581] loss=4.4425, lr=0.0000391433, Top3-acc=0.976\n",
      "[Epoch 6 Batch 2400/2581] loss=4.9292, lr=0.0000390917, Top3-acc=0.976\n",
      "[Epoch 6 Batch 2450/2581] loss=4.3136, lr=0.0000390357, Top3-acc=0.976\n",
      "[Epoch 6 Batch 2500/2581] loss=5.2449, lr=0.0000389841, Top3-acc=0.976\n",
      "[Epoch 6 Batch 2550/2581] loss=4.5695, lr=0.0000389281, Top3-acc=0.976\n",
      "validation Top3-Acc : 0.9141331589844176\n",
      "[Epoch 7 Batch 50/2581] loss=4.9251, lr=0.0000388420, Top3-acc=0.974\n",
      "[Epoch 7 Batch 100/2581] loss=4.5253, lr=0.0000387904, Top3-acc=0.977\n",
      "[Epoch 7 Batch 150/2581] loss=4.9080, lr=0.0000387344, Top3-acc=0.979\n",
      "[Epoch 7 Batch 200/2581] loss=5.3797, lr=0.0000386827, Top3-acc=0.979\n",
      "[Epoch 7 Batch 250/2581] loss=4.5542, lr=0.0000386268, Top3-acc=0.979\n",
      "[Epoch 7 Batch 300/2581] loss=4.9089, lr=0.0000385751, Top3-acc=0.978\n",
      "[Epoch 7 Batch 350/2581] loss=4.5639, lr=0.0000385192, Top3-acc=0.979\n",
      "[Epoch 7 Batch 400/2581] loss=4.9554, lr=0.0000384675, Top3-acc=0.978\n",
      "[Epoch 7 Batch 450/2581] loss=4.9697, lr=0.0000384115, Top3-acc=0.977\n",
      "[Epoch 7 Batch 500/2581] loss=4.7359, lr=0.0000383599, Top3-acc=0.978\n",
      "[Epoch 7 Batch 550/2581] loss=4.4393, lr=0.0000383039, Top3-acc=0.978\n",
      "[Epoch 7 Batch 600/2581] loss=4.1370, lr=0.0000382523, Top3-acc=0.979\n",
      "[Epoch 7 Batch 650/2581] loss=4.6515, lr=0.0000381963, Top3-acc=0.979\n",
      "[Epoch 7 Batch 700/2581] loss=4.8313, lr=0.0000381446, Top3-acc=0.979\n",
      "[Epoch 7 Batch 750/2581] loss=4.4769, lr=0.0000380887, Top3-acc=0.979\n",
      "[Epoch 7 Batch 800/2581] loss=4.3367, lr=0.0000380370, Top3-acc=0.979\n",
      "[Epoch 7 Batch 850/2581] loss=4.8227, lr=0.0000379811, Top3-acc=0.979\n",
      "[Epoch 7 Batch 900/2581] loss=4.2298, lr=0.0000379294, Top3-acc=0.980\n",
      "[Epoch 7 Batch 950/2581] loss=4.0809, lr=0.0000378734, Top3-acc=0.980\n",
      "[Epoch 7 Batch 1000/2581] loss=3.9874, lr=0.0000378218, Top3-acc=0.981\n",
      "[Epoch 7 Batch 1050/2581] loss=3.5911, lr=0.0000377658, Top3-acc=0.981\n",
      "[Epoch 7 Batch 1100/2581] loss=4.2901, lr=0.0000377142, Top3-acc=0.981\n",
      "[Epoch 7 Batch 1150/2581] loss=3.9940, lr=0.0000376582, Top3-acc=0.981\n",
      "[Epoch 7 Batch 1200/2581] loss=4.1988, lr=0.0000376065, Top3-acc=0.981\n",
      "[Epoch 7 Batch 1250/2581] loss=4.0034, lr=0.0000375506, Top3-acc=0.981\n",
      "[Epoch 7 Batch 1300/2581] loss=4.3134, lr=0.0000374989, Top3-acc=0.982\n",
      "[Epoch 7 Batch 1350/2581] loss=4.7863, lr=0.0000374430, Top3-acc=0.981\n",
      "[Epoch 7 Batch 1400/2581] loss=4.0845, lr=0.0000373913, Top3-acc=0.982\n",
      "[Epoch 7 Batch 1450/2581] loss=4.7658, lr=0.0000373353, Top3-acc=0.982\n",
      "[Epoch 7 Batch 1500/2581] loss=3.6963, lr=0.0000372837, Top3-acc=0.982\n",
      "[Epoch 7 Batch 1550/2581] loss=4.4623, lr=0.0000372277, Top3-acc=0.982\n",
      "[Epoch 7 Batch 1600/2581] loss=4.5041, lr=0.0000371761, Top3-acc=0.982\n",
      "[Epoch 7 Batch 1650/2581] loss=4.5319, lr=0.0000371201, Top3-acc=0.982\n",
      "[Epoch 7 Batch 1700/2581] loss=4.3462, lr=0.0000370684, Top3-acc=0.982\n",
      "[Epoch 7 Batch 1750/2581] loss=3.6526, lr=0.0000370125, Top3-acc=0.982\n",
      "[Epoch 7 Batch 1800/2581] loss=3.7148, lr=0.0000369608, Top3-acc=0.982\n",
      "[Epoch 7 Batch 1850/2581] loss=3.9923, lr=0.0000369049, Top3-acc=0.982\n",
      "[Epoch 7 Batch 1900/2581] loss=4.2671, lr=0.0000368532, Top3-acc=0.982\n",
      "[Epoch 7 Batch 1950/2581] loss=3.5661, lr=0.0000367972, Top3-acc=0.982\n",
      "[Epoch 7 Batch 2000/2581] loss=4.1125, lr=0.0000367456, Top3-acc=0.982\n",
      "[Epoch 7 Batch 2050/2581] loss=4.1564, lr=0.0000366896, Top3-acc=0.982\n",
      "[Epoch 7 Batch 2100/2581] loss=3.2911, lr=0.0000366380, Top3-acc=0.982\n",
      "[Epoch 7 Batch 2150/2581] loss=4.9482, lr=0.0000365820, Top3-acc=0.982\n",
      "[Epoch 7 Batch 2200/2581] loss=3.6331, lr=0.0000365303, Top3-acc=0.982\n",
      "[Epoch 7 Batch 2250/2581] loss=3.6323, lr=0.0000364744, Top3-acc=0.982\n",
      "[Epoch 7 Batch 2300/2581] loss=3.5040, lr=0.0000364227, Top3-acc=0.983\n",
      "[Epoch 7 Batch 2350/2581] loss=3.8106, lr=0.0000363668, Top3-acc=0.983\n",
      "[Epoch 7 Batch 2400/2581] loss=3.9236, lr=0.0000363151, Top3-acc=0.983\n",
      "[Epoch 7 Batch 2450/2581] loss=3.7758, lr=0.0000362591, Top3-acc=0.983\n",
      "[Epoch 7 Batch 2500/2581] loss=4.3913, lr=0.0000362075, Top3-acc=0.983\n",
      "[Epoch 7 Batch 2550/2581] loss=3.8922, lr=0.0000361515, Top3-acc=0.983\n",
      "validation Top3-Acc : 0.9102103083796448\n",
      "[Epoch 8 Batch 50/2581] loss=4.4312, lr=0.0000360654, Top3-acc=0.978\n",
      "[Epoch 8 Batch 100/2581] loss=4.1200, lr=0.0000360138, Top3-acc=0.981\n",
      "[Epoch 8 Batch 150/2581] loss=4.0321, lr=0.0000359578, Top3-acc=0.983\n",
      "[Epoch 8 Batch 200/2581] loss=4.2440, lr=0.0000359062, Top3-acc=0.984\n",
      "[Epoch 8 Batch 250/2581] loss=3.7436, lr=0.0000358502, Top3-acc=0.984\n",
      "[Epoch 8 Batch 300/2581] loss=3.8043, lr=0.0000357985, Top3-acc=0.984\n",
      "[Epoch 8 Batch 350/2581] loss=3.3975, lr=0.0000357426, Top3-acc=0.985\n",
      "[Epoch 8 Batch 400/2581] loss=4.3180, lr=0.0000356909, Top3-acc=0.984\n",
      "[Epoch 8 Batch 450/2581] loss=3.9260, lr=0.0000356350, Top3-acc=0.984\n",
      "[Epoch 8 Batch 500/2581] loss=3.6719, lr=0.0000355833, Top3-acc=0.985\n",
      "[Epoch 8 Batch 550/2581] loss=3.8839, lr=0.0000355273, Top3-acc=0.984\n",
      "[Epoch 8 Batch 600/2581] loss=3.3073, lr=0.0000354757, Top3-acc=0.985\n",
      "[Epoch 8 Batch 650/2581] loss=3.7348, lr=0.0000354197, Top3-acc=0.985\n",
      "[Epoch 8 Batch 700/2581] loss=3.8228, lr=0.0000353681, Top3-acc=0.985\n",
      "[Epoch 8 Batch 750/2581] loss=3.5652, lr=0.0000353121, Top3-acc=0.985\n",
      "[Epoch 8 Batch 800/2581] loss=3.4498, lr=0.0000352604, Top3-acc=0.985\n",
      "[Epoch 8 Batch 850/2581] loss=3.7104, lr=0.0000352045, Top3-acc=0.985\n",
      "[Epoch 8 Batch 900/2581] loss=3.3668, lr=0.0000351528, Top3-acc=0.986\n",
      "[Epoch 8 Batch 950/2581] loss=3.2548, lr=0.0000350969, Top3-acc=0.986\n",
      "[Epoch 8 Batch 1000/2581] loss=3.3563, lr=0.0000350452, Top3-acc=0.986\n",
      "[Epoch 8 Batch 1050/2581] loss=2.9929, lr=0.0000349892, Top3-acc=0.986\n",
      "[Epoch 8 Batch 1100/2581] loss=3.6075, lr=0.0000349376, Top3-acc=0.986\n",
      "[Epoch 8 Batch 1150/2581] loss=3.4331, lr=0.0000348816, Top3-acc=0.986\n",
      "[Epoch 8 Batch 1200/2581] loss=3.5375, lr=0.0000348300, Top3-acc=0.986\n",
      "[Epoch 8 Batch 1250/2581] loss=3.1237, lr=0.0000347740, Top3-acc=0.987\n",
      "[Epoch 8 Batch 1300/2581] loss=3.2866, lr=0.0000347223, Top3-acc=0.987\n",
      "[Epoch 8 Batch 1350/2581] loss=3.8368, lr=0.0000346664, Top3-acc=0.987\n",
      "[Epoch 8 Batch 1400/2581] loss=3.0103, lr=0.0000346147, Top3-acc=0.987\n",
      "[Epoch 8 Batch 1450/2581] loss=3.8559, lr=0.0000345588, Top3-acc=0.987\n",
      "[Epoch 8 Batch 1500/2581] loss=2.8788, lr=0.0000345071, Top3-acc=0.987\n",
      "[Epoch 8 Batch 1550/2581] loss=3.3946, lr=0.0000344511, Top3-acc=0.987\n",
      "[Epoch 8 Batch 1600/2581] loss=3.8154, lr=0.0000343995, Top3-acc=0.987\n",
      "[Epoch 8 Batch 1650/2581] loss=3.4819, lr=0.0000343435, Top3-acc=0.987\n",
      "[Epoch 8 Batch 1700/2581] loss=3.7055, lr=0.0000342919, Top3-acc=0.987\n",
      "[Epoch 8 Batch 1750/2581] loss=2.9601, lr=0.0000342359, Top3-acc=0.987\n",
      "[Epoch 8 Batch 1800/2581] loss=3.0914, lr=0.0000341842, Top3-acc=0.987\n",
      "[Epoch 8 Batch 1850/2581] loss=3.3487, lr=0.0000341283, Top3-acc=0.987\n",
      "[Epoch 8 Batch 1900/2581] loss=3.2441, lr=0.0000340766, Top3-acc=0.987\n",
      "[Epoch 8 Batch 1950/2581] loss=3.1798, lr=0.0000340207, Top3-acc=0.987\n",
      "[Epoch 8 Batch 2000/2581] loss=3.4014, lr=0.0000339690, Top3-acc=0.987\n",
      "[Epoch 8 Batch 2050/2581] loss=3.5038, lr=0.0000339130, Top3-acc=0.987\n",
      "[Epoch 8 Batch 2100/2581] loss=2.8256, lr=0.0000338614, Top3-acc=0.987\n",
      "[Epoch 8 Batch 2150/2581] loss=3.9055, lr=0.0000338054, Top3-acc=0.987\n",
      "[Epoch 8 Batch 2200/2581] loss=2.8361, lr=0.0000337538, Top3-acc=0.987\n",
      "[Epoch 8 Batch 2250/2581] loss=2.8563, lr=0.0000336978, Top3-acc=0.988\n",
      "[Epoch 8 Batch 2300/2581] loss=3.0109, lr=0.0000336461, Top3-acc=0.988\n",
      "[Epoch 8 Batch 2350/2581] loss=3.2649, lr=0.0000335902, Top3-acc=0.988\n",
      "[Epoch 8 Batch 2400/2581] loss=3.2064, lr=0.0000335385, Top3-acc=0.988\n",
      "[Epoch 8 Batch 2450/2581] loss=3.3362, lr=0.0000334826, Top3-acc=0.988\n",
      "[Epoch 8 Batch 2500/2581] loss=4.1288, lr=0.0000334309, Top3-acc=0.988\n",
      "[Epoch 8 Batch 2550/2581] loss=3.1133, lr=0.0000333749, Top3-acc=0.988\n",
      "validation Top3-Acc : 0.9098834041625804\n",
      "[Epoch 9 Batch 50/2581] loss=3.5555, lr=0.0000332889, Top3-acc=0.986\n",
      "[Epoch 9 Batch 100/2581] loss=3.2491, lr=0.0000332372, Top3-acc=0.986\n",
      "[Epoch 9 Batch 150/2581] loss=3.2869, lr=0.0000331812, Top3-acc=0.987\n",
      "[Epoch 9 Batch 200/2581] loss=3.4812, lr=0.0000331296, Top3-acc=0.988\n",
      "[Epoch 9 Batch 250/2581] loss=3.1137, lr=0.0000330736, Top3-acc=0.989\n",
      "[Epoch 9 Batch 300/2581] loss=3.2326, lr=0.0000330220, Top3-acc=0.989\n",
      "[Epoch 9 Batch 350/2581] loss=2.9677, lr=0.0000329660, Top3-acc=0.989\n",
      "[Epoch 9 Batch 400/2581] loss=3.4635, lr=0.0000329143, Top3-acc=0.989\n",
      "[Epoch 9 Batch 450/2581] loss=3.3914, lr=0.0000328584, Top3-acc=0.989\n",
      "[Epoch 9 Batch 500/2581] loss=2.9978, lr=0.0000328067, Top3-acc=0.989\n",
      "[Epoch 9 Batch 550/2581] loss=3.1101, lr=0.0000327508, Top3-acc=0.989\n",
      "[Epoch 9 Batch 600/2581] loss=2.7985, lr=0.0000326991, Top3-acc=0.990\n",
      "[Epoch 9 Batch 650/2581] loss=3.1155, lr=0.0000326431, Top3-acc=0.990\n",
      "[Epoch 9 Batch 700/2581] loss=3.3578, lr=0.0000325915, Top3-acc=0.990\n",
      "[Epoch 9 Batch 750/2581] loss=2.8841, lr=0.0000325355, Top3-acc=0.990\n",
      "[Epoch 9 Batch 800/2581] loss=2.9857, lr=0.0000324839, Top3-acc=0.990\n",
      "[Epoch 9 Batch 850/2581] loss=3.4033, lr=0.0000324279, Top3-acc=0.990\n",
      "[Epoch 9 Batch 900/2581] loss=2.7591, lr=0.0000323762, Top3-acc=0.990\n",
      "[Epoch 9 Batch 950/2581] loss=2.6240, lr=0.0000323203, Top3-acc=0.990\n",
      "[Epoch 9 Batch 1000/2581] loss=2.7615, lr=0.0000322686, Top3-acc=0.990\n",
      "[Epoch 9 Batch 1050/2581] loss=2.3304, lr=0.0000322127, Top3-acc=0.990\n",
      "[Epoch 9 Batch 1100/2581] loss=2.6009, lr=0.0000321610, Top3-acc=0.990\n",
      "[Epoch 9 Batch 1150/2581] loss=2.7410, lr=0.0000321050, Top3-acc=0.990\n",
      "[Epoch 9 Batch 1200/2581] loss=3.0787, lr=0.0000320534, Top3-acc=0.990\n",
      "[Epoch 9 Batch 1250/2581] loss=2.6367, lr=0.0000319974, Top3-acc=0.990\n",
      "[Epoch 9 Batch 1300/2581] loss=2.9914, lr=0.0000319458, Top3-acc=0.991\n",
      "[Epoch 9 Batch 1350/2581] loss=2.8326, lr=0.0000318898, Top3-acc=0.991\n",
      "[Epoch 9 Batch 1400/2581] loss=2.1922, lr=0.0000318381, Top3-acc=0.991\n",
      "[Epoch 9 Batch 1450/2581] loss=3.0821, lr=0.0000317822, Top3-acc=0.991\n",
      "[Epoch 9 Batch 1500/2581] loss=2.3645, lr=0.0000317305, Top3-acc=0.991\n",
      "[Epoch 9 Batch 1550/2581] loss=2.9881, lr=0.0000316746, Top3-acc=0.991\n",
      "[Epoch 9 Batch 1600/2581] loss=3.1477, lr=0.0000316229, Top3-acc=0.991\n",
      "[Epoch 9 Batch 1650/2581] loss=2.8618, lr=0.0000315669, Top3-acc=0.991\n",
      "[Epoch 9 Batch 1700/2581] loss=2.8013, lr=0.0000315153, Top3-acc=0.991\n",
      "[Epoch 9 Batch 1750/2581] loss=2.1928, lr=0.0000314593, Top3-acc=0.991\n",
      "[Epoch 9 Batch 1800/2581] loss=2.2875, lr=0.0000314077, Top3-acc=0.991\n",
      "[Epoch 9 Batch 1850/2581] loss=2.6129, lr=0.0000313517, Top3-acc=0.991\n",
      "[Epoch 9 Batch 1900/2581] loss=2.8774, lr=0.0000313000, Top3-acc=0.991\n",
      "[Epoch 9 Batch 1950/2581] loss=2.5065, lr=0.0000312441, Top3-acc=0.991\n",
      "[Epoch 9 Batch 2000/2581] loss=2.9117, lr=0.0000311924, Top3-acc=0.991\n",
      "[Epoch 9 Batch 2050/2581] loss=2.9265, lr=0.0000311365, Top3-acc=0.991\n",
      "[Epoch 9 Batch 2100/2581] loss=2.4155, lr=0.0000310848, Top3-acc=0.991\n",
      "[Epoch 9 Batch 2150/2581] loss=3.4606, lr=0.0000310288, Top3-acc=0.991\n",
      "[Epoch 9 Batch 2200/2581] loss=2.2224, lr=0.0000309772, Top3-acc=0.991\n",
      "[Epoch 9 Batch 2250/2581] loss=2.3389, lr=0.0000309212, Top3-acc=0.991\n",
      "[Epoch 9 Batch 2300/2581] loss=2.4356, lr=0.0000308696, Top3-acc=0.991\n",
      "[Epoch 9 Batch 2350/2581] loss=2.3703, lr=0.0000308136, Top3-acc=0.991\n",
      "[Epoch 9 Batch 2400/2581] loss=2.7227, lr=0.0000307619, Top3-acc=0.991\n",
      "[Epoch 9 Batch 2450/2581] loss=2.5479, lr=0.0000307060, Top3-acc=0.991\n",
      "[Epoch 9 Batch 2500/2581] loss=3.2273, lr=0.0000306543, Top3-acc=0.991\n",
      "[Epoch 9 Batch 2550/2581] loss=2.5980, lr=0.0000305984, Top3-acc=0.991\n",
      "validation Top3-Acc : 0.9145690312738368\n",
      "[Epoch 10 Batch 50/2581] loss=2.9090, lr=0.0000305123, Top3-acc=0.990\n",
      "[Epoch 10 Batch 100/2581] loss=2.8969, lr=0.0000304606, Top3-acc=0.992\n",
      "[Epoch 10 Batch 150/2581] loss=2.8384, lr=0.0000304046, Top3-acc=0.992\n",
      "[Epoch 10 Batch 200/2581] loss=2.7607, lr=0.0000303530, Top3-acc=0.993\n",
      "[Epoch 10 Batch 250/2581] loss=2.1881, lr=0.0000302970, Top3-acc=0.993\n",
      "[Epoch 10 Batch 300/2581] loss=2.7367, lr=0.0000302454, Top3-acc=0.993\n",
      "[Epoch 10 Batch 350/2581] loss=2.1744, lr=0.0000301894, Top3-acc=0.993\n",
      "[Epoch 10 Batch 400/2581] loss=2.8296, lr=0.0000301378, Top3-acc=0.993\n",
      "[Epoch 10 Batch 450/2581] loss=2.6987, lr=0.0000300818, Top3-acc=0.993\n",
      "[Epoch 10 Batch 500/2581] loss=2.6057, lr=0.0000300301, Top3-acc=0.993\n",
      "[Epoch 10 Batch 550/2581] loss=2.7191, lr=0.0000299742, Top3-acc=0.993\n",
      "[Epoch 10 Batch 600/2581] loss=2.2717, lr=0.0000299225, Top3-acc=0.993\n",
      "[Epoch 10 Batch 650/2581] loss=2.6036, lr=0.0000298666, Top3-acc=0.993\n",
      "[Epoch 10 Batch 700/2581] loss=2.7057, lr=0.0000298149, Top3-acc=0.993\n",
      "[Epoch 10 Batch 750/2581] loss=2.5456, lr=0.0000297589, Top3-acc=0.993\n",
      "[Epoch 10 Batch 800/2581] loss=2.4223, lr=0.0000297073, Top3-acc=0.993\n",
      "[Epoch 10 Batch 850/2581] loss=2.7989, lr=0.0000296513, Top3-acc=0.993\n",
      "[Epoch 10 Batch 900/2581] loss=2.1305, lr=0.0000295997, Top3-acc=0.993\n",
      "[Epoch 10 Batch 950/2581] loss=2.0230, lr=0.0000295437, Top3-acc=0.993\n",
      "[Epoch 10 Batch 1000/2581] loss=2.3078, lr=0.0000294920, Top3-acc=0.993\n",
      "[Epoch 10 Batch 1050/2581] loss=1.9334, lr=0.0000294361, Top3-acc=0.993\n",
      "[Epoch 10 Batch 1100/2581] loss=2.1731, lr=0.0000293844, Top3-acc=0.993\n",
      "[Epoch 10 Batch 1150/2581] loss=2.1470, lr=0.0000293285, Top3-acc=0.994\n",
      "[Epoch 10 Batch 1200/2581] loss=2.3764, lr=0.0000292768, Top3-acc=0.994\n",
      "[Epoch 10 Batch 1250/2581] loss=2.1989, lr=0.0000292208, Top3-acc=0.994\n",
      "[Epoch 10 Batch 1300/2581] loss=2.2596, lr=0.0000291692, Top3-acc=0.994\n",
      "[Epoch 10 Batch 1350/2581] loss=2.4954, lr=0.0000291132, Top3-acc=0.994\n",
      "[Epoch 10 Batch 1400/2581] loss=2.0145, lr=0.0000290616, Top3-acc=0.994\n",
      "[Epoch 10 Batch 1450/2581] loss=2.5379, lr=0.0000290056, Top3-acc=0.994\n",
      "[Epoch 10 Batch 1500/2581] loss=1.9892, lr=0.0000289539, Top3-acc=0.994\n",
      "[Epoch 10 Batch 1550/2581] loss=2.7058, lr=0.0000288980, Top3-acc=0.994\n",
      "[Epoch 10 Batch 1600/2581] loss=2.6968, lr=0.0000288463, Top3-acc=0.994\n",
      "[Epoch 10 Batch 1650/2581] loss=2.2899, lr=0.0000287904, Top3-acc=0.994\n",
      "[Epoch 10 Batch 1700/2581] loss=2.4665, lr=0.0000287387, Top3-acc=0.994\n",
      "[Epoch 10 Batch 1750/2581] loss=1.9251, lr=0.0000286827, Top3-acc=0.994\n",
      "[Epoch 10 Batch 1800/2581] loss=1.8950, lr=0.0000286311, Top3-acc=0.994\n",
      "[Epoch 10 Batch 1850/2581] loss=1.9416, lr=0.0000285751, Top3-acc=0.994\n",
      "[Epoch 10 Batch 1900/2581] loss=2.2003, lr=0.0000285235, Top3-acc=0.994\n",
      "[Epoch 10 Batch 1950/2581] loss=2.1383, lr=0.0000284675, Top3-acc=0.994\n",
      "[Epoch 10 Batch 2000/2581] loss=2.2404, lr=0.0000284158, Top3-acc=0.994\n",
      "[Epoch 10 Batch 2050/2581] loss=2.5945, lr=0.0000283599, Top3-acc=0.994\n",
      "[Epoch 10 Batch 2100/2581] loss=2.2195, lr=0.0000283082, Top3-acc=0.994\n",
      "[Epoch 10 Batch 2150/2581] loss=2.7550, lr=0.0000282523, Top3-acc=0.994\n",
      "[Epoch 10 Batch 2200/2581] loss=1.8130, lr=0.0000282006, Top3-acc=0.994\n",
      "[Epoch 10 Batch 2250/2581] loss=2.0367, lr=0.0000281446, Top3-acc=0.994\n",
      "[Epoch 10 Batch 2300/2581] loss=1.8686, lr=0.0000280930, Top3-acc=0.994\n",
      "[Epoch 10 Batch 2350/2581] loss=2.1513, lr=0.0000280370, Top3-acc=0.994\n",
      "[Epoch 10 Batch 2400/2581] loss=2.2580, lr=0.0000279854, Top3-acc=0.994\n",
      "[Epoch 10 Batch 2450/2581] loss=2.1307, lr=0.0000279294, Top3-acc=0.994\n",
      "[Epoch 10 Batch 2500/2581] loss=2.6205, lr=0.0000278777, Top3-acc=0.994\n",
      "[Epoch 10 Batch 2550/2581] loss=2.0590, lr=0.0000278218, Top3-acc=0.994\n",
      "validation Top3-Acc : 0.911735861392612\n",
      "[Epoch 11 Batch 50/2581] loss=2.2854, lr=0.0000277357, Top3-acc=0.992\n",
      "[Epoch 11 Batch 100/2581] loss=1.9661, lr=0.0000276840, Top3-acc=0.993\n",
      "[Epoch 11 Batch 150/2581] loss=2.1200, lr=0.0000276281, Top3-acc=0.995\n",
      "[Epoch 11 Batch 200/2581] loss=2.3131, lr=0.0000275764, Top3-acc=0.995\n",
      "[Epoch 11 Batch 250/2581] loss=1.7851, lr=0.0000275204, Top3-acc=0.996\n",
      "[Epoch 11 Batch 300/2581] loss=1.9509, lr=0.0000274688, Top3-acc=0.995\n",
      "[Epoch 11 Batch 350/2581] loss=1.8032, lr=0.0000274128, Top3-acc=0.995\n",
      "[Epoch 11 Batch 400/2581] loss=2.0349, lr=0.0000273612, Top3-acc=0.995\n",
      "[Epoch 11 Batch 450/2581] loss=2.2068, lr=0.0000273052, Top3-acc=0.995\n",
      "[Epoch 11 Batch 500/2581] loss=2.1550, lr=0.0000272536, Top3-acc=0.995\n",
      "[Epoch 11 Batch 550/2581] loss=2.0403, lr=0.0000271976, Top3-acc=0.995\n",
      "[Epoch 11 Batch 600/2581] loss=1.9405, lr=0.0000271459, Top3-acc=0.995\n",
      "[Epoch 11 Batch 650/2581] loss=2.1931, lr=0.0000270900, Top3-acc=0.995\n",
      "[Epoch 11 Batch 700/2581] loss=2.2947, lr=0.0000270383, Top3-acc=0.995\n",
      "[Epoch 11 Batch 750/2581] loss=2.1040, lr=0.0000269824, Top3-acc=0.995\n",
      "[Epoch 11 Batch 800/2581] loss=1.8422, lr=0.0000269307, Top3-acc=0.995\n",
      "[Epoch 11 Batch 850/2581] loss=1.9719, lr=0.0000268747, Top3-acc=0.995\n",
      "[Epoch 11 Batch 900/2581] loss=1.7054, lr=0.0000268231, Top3-acc=0.995\n",
      "[Epoch 11 Batch 950/2581] loss=1.7975, lr=0.0000267671, Top3-acc=0.995\n",
      "[Epoch 11 Batch 1000/2581] loss=1.7489, lr=0.0000267155, Top3-acc=0.995\n",
      "[Epoch 11 Batch 1050/2581] loss=1.4369, lr=0.0000266595, Top3-acc=0.995\n",
      "[Epoch 11 Batch 1100/2581] loss=1.8320, lr=0.0000266078, Top3-acc=0.995\n",
      "[Epoch 11 Batch 1150/2581] loss=1.8658, lr=0.0000265519, Top3-acc=0.995\n",
      "[Epoch 11 Batch 1200/2581] loss=1.9303, lr=0.0000265002, Top3-acc=0.995\n",
      "[Epoch 11 Batch 1250/2581] loss=1.7648, lr=0.0000264443, Top3-acc=0.995\n",
      "[Epoch 11 Batch 1300/2581] loss=1.7451, lr=0.0000263926, Top3-acc=0.995\n",
      "[Epoch 11 Batch 1350/2581] loss=1.9501, lr=0.0000263366, Top3-acc=0.995\n",
      "[Epoch 11 Batch 1400/2581] loss=1.6560, lr=0.0000262850, Top3-acc=0.995\n",
      "[Epoch 11 Batch 1450/2581] loss=2.2617, lr=0.0000262290, Top3-acc=0.995\n",
      "[Epoch 11 Batch 1500/2581] loss=1.7821, lr=0.0000261774, Top3-acc=0.995\n",
      "[Epoch 11 Batch 1550/2581] loss=2.1798, lr=0.0000261214, Top3-acc=0.995\n",
      "[Epoch 11 Batch 1600/2581] loss=2.2965, lr=0.0000260697, Top3-acc=0.995\n",
      "[Epoch 11 Batch 1650/2581] loss=2.1006, lr=0.0000260138, Top3-acc=0.995\n",
      "[Epoch 11 Batch 1700/2581] loss=1.8526, lr=0.0000259621, Top3-acc=0.995\n",
      "[Epoch 11 Batch 1750/2581] loss=1.6498, lr=0.0000259062, Top3-acc=0.995\n",
      "[Epoch 11 Batch 1800/2581] loss=1.5678, lr=0.0000258545, Top3-acc=0.995\n",
      "[Epoch 11 Batch 1850/2581] loss=1.7834, lr=0.0000257985, Top3-acc=0.995\n",
      "[Epoch 11 Batch 1900/2581] loss=1.7298, lr=0.0000257469, Top3-acc=0.995\n",
      "[Epoch 11 Batch 1950/2581] loss=1.8167, lr=0.0000256909, Top3-acc=0.995\n",
      "[Epoch 11 Batch 2000/2581] loss=1.8895, lr=0.0000256393, Top3-acc=0.995\n",
      "[Epoch 11 Batch 2050/2581] loss=2.0217, lr=0.0000255833, Top3-acc=0.996\n",
      "[Epoch 11 Batch 2100/2581] loss=1.6672, lr=0.0000255316, Top3-acc=0.996\n",
      "[Epoch 11 Batch 2150/2581] loss=2.3021, lr=0.0000254757, Top3-acc=0.996\n",
      "[Epoch 11 Batch 2200/2581] loss=1.6834, lr=0.0000254240, Top3-acc=0.995\n",
      "[Epoch 11 Batch 2250/2581] loss=1.6139, lr=0.0000253681, Top3-acc=0.996\n",
      "[Epoch 11 Batch 2300/2581] loss=1.6274, lr=0.0000253164, Top3-acc=0.996\n",
      "[Epoch 11 Batch 2350/2581] loss=1.5429, lr=0.0000252604, Top3-acc=0.996\n",
      "[Epoch 11 Batch 2400/2581] loss=1.7220, lr=0.0000252088, Top3-acc=0.996\n",
      "[Epoch 11 Batch 2450/2581] loss=1.6993, lr=0.0000251528, Top3-acc=0.996\n",
      "[Epoch 11 Batch 2500/2581] loss=2.3222, lr=0.0000251012, Top3-acc=0.996\n",
      "[Epoch 11 Batch 2550/2581] loss=1.7827, lr=0.0000250452, Top3-acc=0.996\n",
      "validation Top3-Acc : 0.9103192764519996\n",
      "[Epoch 12 Batch 50/2581] loss=1.9812, lr=0.0000249591, Top3-acc=0.993\n",
      "[Epoch 12 Batch 100/2581] loss=1.8557, lr=0.0000249074, Top3-acc=0.995\n",
      "[Epoch 12 Batch 150/2581] loss=1.7934, lr=0.0000248515, Top3-acc=0.996\n",
      "[Epoch 12 Batch 200/2581] loss=1.7713, lr=0.0000247998, Top3-acc=0.996\n",
      "[Epoch 12 Batch 250/2581] loss=1.3637, lr=0.0000247439, Top3-acc=0.997\n",
      "[Epoch 12 Batch 300/2581] loss=1.8134, lr=0.0000246922, Top3-acc=0.996\n",
      "[Epoch 12 Batch 350/2581] loss=1.4285, lr=0.0000246362, Top3-acc=0.996\n",
      "[Epoch 12 Batch 400/2581] loss=1.6085, lr=0.0000245846, Top3-acc=0.996\n",
      "[Epoch 12 Batch 450/2581] loss=1.8818, lr=0.0000245286, Top3-acc=0.996\n",
      "[Epoch 12 Batch 500/2581] loss=1.5700, lr=0.0000244770, Top3-acc=0.996\n",
      "[Epoch 12 Batch 550/2581] loss=2.1748, lr=0.0000244210, Top3-acc=0.996\n",
      "[Epoch 12 Batch 600/2581] loss=1.5137, lr=0.0000243693, Top3-acc=0.996\n",
      "[Epoch 12 Batch 650/2581] loss=1.8803, lr=0.0000243134, Top3-acc=0.996\n",
      "[Epoch 12 Batch 700/2581] loss=2.1270, lr=0.0000242617, Top3-acc=0.996\n",
      "[Epoch 12 Batch 750/2581] loss=1.7889, lr=0.0000242058, Top3-acc=0.996\n",
      "[Epoch 12 Batch 800/2581] loss=1.5281, lr=0.0000241541, Top3-acc=0.996\n",
      "[Epoch 12 Batch 850/2581] loss=1.6383, lr=0.0000240981, Top3-acc=0.996\n",
      "[Epoch 12 Batch 900/2581] loss=1.4011, lr=0.0000240465, Top3-acc=0.996\n",
      "[Epoch 12 Batch 950/2581] loss=1.4068, lr=0.0000239905, Top3-acc=0.996\n",
      "[Epoch 12 Batch 1000/2581] loss=1.3959, lr=0.0000239389, Top3-acc=0.996\n",
      "[Epoch 12 Batch 1050/2581] loss=1.0820, lr=0.0000238829, Top3-acc=0.996\n",
      "[Epoch 12 Batch 1100/2581] loss=1.5126, lr=0.0000238313, Top3-acc=0.996\n",
      "[Epoch 12 Batch 1150/2581] loss=1.3915, lr=0.0000237753, Top3-acc=0.996\n",
      "[Epoch 12 Batch 1200/2581] loss=1.7772, lr=0.0000237236, Top3-acc=0.996\n",
      "[Epoch 12 Batch 1250/2581] loss=1.6312, lr=0.0000236677, Top3-acc=0.996\n",
      "[Epoch 12 Batch 1300/2581] loss=1.6288, lr=0.0000236160, Top3-acc=0.996\n",
      "[Epoch 12 Batch 1350/2581] loss=1.5765, lr=0.0000235601, Top3-acc=0.996\n",
      "[Epoch 12 Batch 1400/2581] loss=1.3158, lr=0.0000235084, Top3-acc=0.997\n",
      "[Epoch 12 Batch 1450/2581] loss=1.7755, lr=0.0000234524, Top3-acc=0.997\n",
      "[Epoch 12 Batch 1500/2581] loss=1.3935, lr=0.0000234008, Top3-acc=0.997\n",
      "[Epoch 12 Batch 1550/2581] loss=1.8953, lr=0.0000233448, Top3-acc=0.996\n",
      "[Epoch 12 Batch 1600/2581] loss=2.1173, lr=0.0000232932, Top3-acc=0.996\n",
      "[Epoch 12 Batch 1650/2581] loss=1.7934, lr=0.0000232372, Top3-acc=0.996\n",
      "[Epoch 12 Batch 1700/2581] loss=1.7162, lr=0.0000231855, Top3-acc=0.996\n",
      "[Epoch 12 Batch 1750/2581] loss=1.4220, lr=0.0000231296, Top3-acc=0.996\n",
      "[Epoch 12 Batch 1800/2581] loss=1.1769, lr=0.0000230779, Top3-acc=0.996\n",
      "[Epoch 12 Batch 1850/2581] loss=1.4920, lr=0.0000230220, Top3-acc=0.996\n",
      "[Epoch 12 Batch 1900/2581] loss=1.3487, lr=0.0000229703, Top3-acc=0.997\n",
      "[Epoch 12 Batch 1950/2581] loss=1.4923, lr=0.0000229143, Top3-acc=0.997\n",
      "[Epoch 12 Batch 2000/2581] loss=1.5258, lr=0.0000228627, Top3-acc=0.997\n",
      "[Epoch 12 Batch 2050/2581] loss=1.7490, lr=0.0000228067, Top3-acc=0.997\n",
      "[Epoch 12 Batch 2100/2581] loss=1.4888, lr=0.0000227551, Top3-acc=0.997\n",
      "[Epoch 12 Batch 2150/2581] loss=2.0588, lr=0.0000226991, Top3-acc=0.997\n",
      "[Epoch 12 Batch 2200/2581] loss=1.3127, lr=0.0000226474, Top3-acc=0.997\n",
      "[Epoch 12 Batch 2250/2581] loss=1.2051, lr=0.0000225915, Top3-acc=0.997\n",
      "[Epoch 12 Batch 2300/2581] loss=1.2039, lr=0.0000225398, Top3-acc=0.997\n",
      "[Epoch 12 Batch 2350/2581] loss=1.3309, lr=0.0000224839, Top3-acc=0.997\n",
      "[Epoch 12 Batch 2400/2581] loss=1.3655, lr=0.0000224322, Top3-acc=0.997\n",
      "[Epoch 12 Batch 2450/2581] loss=1.3813, lr=0.0000223762, Top3-acc=0.997\n",
      "[Epoch 12 Batch 2500/2581] loss=2.0605, lr=0.0000223246, Top3-acc=0.997\n",
      "[Epoch 12 Batch 2550/2581] loss=1.5475, lr=0.0000222686, Top3-acc=0.997\n",
      "validation Top3-Acc : 0.9078130107878392\n",
      "[Epoch 13 Batch 50/2581] loss=1.4026, lr=0.0000221825, Top3-acc=0.996\n",
      "[Epoch 13 Batch 100/2581] loss=1.4316, lr=0.0000221309, Top3-acc=0.997\n",
      "[Epoch 13 Batch 150/2581] loss=1.4481, lr=0.0000220749, Top3-acc=0.997\n",
      "[Epoch 13 Batch 200/2581] loss=1.6784, lr=0.0000220232, Top3-acc=0.997\n",
      "[Epoch 13 Batch 250/2581] loss=1.2451, lr=0.0000219673, Top3-acc=0.998\n",
      "[Epoch 13 Batch 300/2581] loss=1.3770, lr=0.0000219156, Top3-acc=0.997\n",
      "[Epoch 13 Batch 350/2581] loss=1.5652, lr=0.0000218597, Top3-acc=0.997\n",
      "[Epoch 13 Batch 400/2581] loss=1.4449, lr=0.0000218080, Top3-acc=0.997\n",
      "[Epoch 13 Batch 450/2581] loss=1.5142, lr=0.0000217520, Top3-acc=0.997\n",
      "[Epoch 13 Batch 500/2581] loss=1.3683, lr=0.0000217004, Top3-acc=0.997\n",
      "[Epoch 13 Batch 550/2581] loss=1.4574, lr=0.0000216444, Top3-acc=0.997\n",
      "[Epoch 13 Batch 600/2581] loss=1.3077, lr=0.0000215928, Top3-acc=0.997\n",
      "[Epoch 13 Batch 650/2581] loss=1.5634, lr=0.0000215368, Top3-acc=0.997\n",
      "[Epoch 13 Batch 700/2581] loss=1.8084, lr=0.0000214851, Top3-acc=0.997\n",
      "[Epoch 13 Batch 750/2581] loss=1.3198, lr=0.0000214292, Top3-acc=0.997\n",
      "[Epoch 13 Batch 800/2581] loss=1.3708, lr=0.0000213775, Top3-acc=0.997\n",
      "[Epoch 13 Batch 850/2581] loss=1.5700, lr=0.0000213216, Top3-acc=0.997\n",
      "[Epoch 13 Batch 900/2581] loss=0.9735, lr=0.0000212699, Top3-acc=0.997\n",
      "[Epoch 13 Batch 950/2581] loss=1.1759, lr=0.0000212139, Top3-acc=0.997\n",
      "[Epoch 13 Batch 1000/2581] loss=1.0238, lr=0.0000211623, Top3-acc=0.997\n",
      "[Epoch 13 Batch 1050/2581] loss=0.9242, lr=0.0000211063, Top3-acc=0.997\n",
      "[Epoch 13 Batch 1100/2581] loss=1.4902, lr=0.0000210547, Top3-acc=0.997\n",
      "[Epoch 13 Batch 1150/2581] loss=1.2777, lr=0.0000209987, Top3-acc=0.997\n",
      "[Epoch 13 Batch 1200/2581] loss=1.4403, lr=0.0000209471, Top3-acc=0.997\n",
      "[Epoch 13 Batch 1250/2581] loss=1.2335, lr=0.0000208911, Top3-acc=0.997\n",
      "[Epoch 13 Batch 1300/2581] loss=1.1727, lr=0.0000208394, Top3-acc=0.997\n",
      "[Epoch 13 Batch 1350/2581] loss=1.3248, lr=0.0000207835, Top3-acc=0.997\n",
      "[Epoch 13 Batch 1400/2581] loss=1.0087, lr=0.0000207318, Top3-acc=0.997\n",
      "[Epoch 13 Batch 1450/2581] loss=1.3690, lr=0.0000206759, Top3-acc=0.997\n",
      "[Epoch 13 Batch 1500/2581] loss=1.3129, lr=0.0000206242, Top3-acc=0.997\n",
      "[Epoch 13 Batch 1550/2581] loss=1.6258, lr=0.0000205682, Top3-acc=0.997\n",
      "[Epoch 13 Batch 1600/2581] loss=1.5101, lr=0.0000205166, Top3-acc=0.997\n",
      "[Epoch 13 Batch 1650/2581] loss=1.5723, lr=0.0000204606, Top3-acc=0.997\n",
      "[Epoch 13 Batch 1700/2581] loss=1.4266, lr=0.0000204090, Top3-acc=0.997\n",
      "[Epoch 13 Batch 1750/2581] loss=0.9942, lr=0.0000203530, Top3-acc=0.997\n",
      "[Epoch 13 Batch 1800/2581] loss=1.0400, lr=0.0000203013, Top3-acc=0.997\n",
      "[Epoch 13 Batch 1850/2581] loss=1.0022, lr=0.0000202454, Top3-acc=0.997\n",
      "[Epoch 13 Batch 1900/2581] loss=1.0596, lr=0.0000201937, Top3-acc=0.998\n",
      "[Epoch 13 Batch 1950/2581] loss=1.2029, lr=0.0000201378, Top3-acc=0.997\n",
      "[Epoch 13 Batch 2000/2581] loss=1.3329, lr=0.0000200861, Top3-acc=0.998\n",
      "[Epoch 13 Batch 2050/2581] loss=1.5554, lr=0.0000200301, Top3-acc=0.998\n",
      "[Epoch 13 Batch 2100/2581] loss=1.1720, lr=0.0000199785, Top3-acc=0.998\n",
      "[Epoch 13 Batch 2150/2581] loss=1.6608, lr=0.0000199225, Top3-acc=0.997\n",
      "[Epoch 13 Batch 2200/2581] loss=1.1972, lr=0.0000198709, Top3-acc=0.997\n",
      "[Epoch 13 Batch 2250/2581] loss=0.9855, lr=0.0000198149, Top3-acc=0.997\n",
      "[Epoch 13 Batch 2300/2581] loss=1.2462, lr=0.0000197632, Top3-acc=0.998\n",
      "[Epoch 13 Batch 2350/2581] loss=1.1109, lr=0.0000197073, Top3-acc=0.998\n",
      "[Epoch 13 Batch 2400/2581] loss=1.1452, lr=0.0000196556, Top3-acc=0.998\n",
      "[Epoch 13 Batch 2450/2581] loss=1.0082, lr=0.0000195997, Top3-acc=0.998\n",
      "[Epoch 13 Batch 2500/2581] loss=1.6910, lr=0.0000195480, Top3-acc=0.998\n",
      "[Epoch 13 Batch 2550/2581] loss=1.2153, lr=0.0000194920, Top3-acc=0.998\n",
      "validation Top3-Acc : 0.9074861065707748\n",
      "[Epoch 14 Batch 50/2581] loss=1.2757, lr=0.0000194059, Top3-acc=0.994\n",
      "[Epoch 14 Batch 100/2581] loss=1.3169, lr=0.0000193543, Top3-acc=0.996\n",
      "[Epoch 14 Batch 150/2581] loss=1.4011, lr=0.0000192983, Top3-acc=0.997\n",
      "[Epoch 14 Batch 200/2581] loss=1.3697, lr=0.0000192467, Top3-acc=0.997\n",
      "[Epoch 14 Batch 250/2581] loss=1.1510, lr=0.0000191907, Top3-acc=0.998\n",
      "[Epoch 14 Batch 300/2581] loss=1.1736, lr=0.0000191390, Top3-acc=0.998\n",
      "[Epoch 14 Batch 350/2581] loss=0.9797, lr=0.0000190831, Top3-acc=0.998\n",
      "[Epoch 14 Batch 400/2581] loss=1.3558, lr=0.0000190314, Top3-acc=0.998\n",
      "[Epoch 14 Batch 450/2581] loss=1.1140, lr=0.0000189755, Top3-acc=0.998\n",
      "[Epoch 14 Batch 500/2581] loss=1.1950, lr=0.0000189238, Top3-acc=0.998\n",
      "[Epoch 14 Batch 550/2581] loss=1.2294, lr=0.0000188678, Top3-acc=0.998\n",
      "[Epoch 14 Batch 600/2581] loss=1.1249, lr=0.0000188162, Top3-acc=0.998\n",
      "[Epoch 14 Batch 650/2581] loss=1.2845, lr=0.0000187602, Top3-acc=0.998\n",
      "[Epoch 14 Batch 700/2581] loss=1.3470, lr=0.0000187086, Top3-acc=0.998\n",
      "[Epoch 14 Batch 750/2581] loss=1.0418, lr=0.0000186526, Top3-acc=0.998\n",
      "[Epoch 14 Batch 800/2581] loss=1.1928, lr=0.0000186009, Top3-acc=0.998\n",
      "[Epoch 14 Batch 850/2581] loss=1.1707, lr=0.0000185450, Top3-acc=0.998\n",
      "[Epoch 14 Batch 900/2581] loss=0.8232, lr=0.0000184933, Top3-acc=0.998\n",
      "[Epoch 14 Batch 950/2581] loss=1.0170, lr=0.0000184374, Top3-acc=0.998\n",
      "[Epoch 14 Batch 1000/2581] loss=0.9439, lr=0.0000183857, Top3-acc=0.998\n",
      "[Epoch 14 Batch 1050/2581] loss=0.6672, lr=0.0000183297, Top3-acc=0.998\n",
      "[Epoch 14 Batch 1100/2581] loss=1.1677, lr=0.0000182781, Top3-acc=0.998\n",
      "[Epoch 14 Batch 1150/2581] loss=0.9054, lr=0.0000182221, Top3-acc=0.998\n",
      "[Epoch 14 Batch 1200/2581] loss=1.2485, lr=0.0000181705, Top3-acc=0.998\n",
      "[Epoch 14 Batch 1250/2581] loss=1.0272, lr=0.0000181145, Top3-acc=0.998\n",
      "[Epoch 14 Batch 1300/2581] loss=1.1128, lr=0.0000180628, Top3-acc=0.998\n",
      "[Epoch 14 Batch 1350/2581] loss=1.0691, lr=0.0000180069, Top3-acc=0.998\n",
      "[Epoch 14 Batch 1400/2581] loss=0.9626, lr=0.0000179552, Top3-acc=0.998\n",
      "[Epoch 14 Batch 1450/2581] loss=1.1206, lr=0.0000178993, Top3-acc=0.998\n",
      "[Epoch 14 Batch 1500/2581] loss=1.2708, lr=0.0000178476, Top3-acc=0.998\n",
      "[Epoch 14 Batch 1550/2581] loss=1.3928, lr=0.0000177916, Top3-acc=0.998\n",
      "[Epoch 14 Batch 1600/2581] loss=1.1775, lr=0.0000177400, Top3-acc=0.998\n",
      "[Epoch 14 Batch 1650/2581] loss=1.5916, lr=0.0000176840, Top3-acc=0.998\n",
      "[Epoch 14 Batch 1700/2581] loss=1.2272, lr=0.0000176324, Top3-acc=0.998\n",
      "[Epoch 14 Batch 1750/2581] loss=0.8772, lr=0.0000175764, Top3-acc=0.998\n",
      "[Epoch 14 Batch 1800/2581] loss=0.7745, lr=0.0000175248, Top3-acc=0.998\n",
      "[Epoch 14 Batch 1850/2581] loss=1.0699, lr=0.0000174688, Top3-acc=0.998\n",
      "[Epoch 14 Batch 1900/2581] loss=0.8991, lr=0.0000174171, Top3-acc=0.998\n",
      "[Epoch 14 Batch 1950/2581] loss=1.0279, lr=0.0000173612, Top3-acc=0.998\n",
      "[Epoch 14 Batch 2000/2581] loss=1.1873, lr=0.0000173095, Top3-acc=0.998\n",
      "[Epoch 14 Batch 2050/2581] loss=1.3926, lr=0.0000172536, Top3-acc=0.998\n",
      "[Epoch 14 Batch 2100/2581] loss=1.1030, lr=0.0000172019, Top3-acc=0.998\n",
      "[Epoch 14 Batch 2150/2581] loss=1.4659, lr=0.0000171459, Top3-acc=0.998\n",
      "[Epoch 14 Batch 2200/2581] loss=1.0166, lr=0.0000170943, Top3-acc=0.998\n",
      "[Epoch 14 Batch 2250/2581] loss=0.8444, lr=0.0000170383, Top3-acc=0.998\n",
      "[Epoch 14 Batch 2300/2581] loss=0.8336, lr=0.0000169867, Top3-acc=0.998\n",
      "[Epoch 14 Batch 2350/2581] loss=0.9711, lr=0.0000169307, Top3-acc=0.998\n",
      "[Epoch 14 Batch 2400/2581] loss=0.9085, lr=0.0000168790, Top3-acc=0.998\n",
      "[Epoch 14 Batch 2450/2581] loss=0.8231, lr=0.0000168231, Top3-acc=0.998\n",
      "[Epoch 14 Batch 2500/2581] loss=1.1677, lr=0.0000167714, Top3-acc=0.998\n",
      "[Epoch 14 Batch 2550/2581] loss=1.0255, lr=0.0000167155, Top3-acc=0.998\n",
      "validation Top3-Acc : 0.9070502342813556\n",
      "[Epoch 15 Batch 50/2581] loss=0.8633, lr=0.0000166294, Top3-acc=0.998\n",
      "[Epoch 15 Batch 100/2581] loss=1.0716, lr=0.0000165777, Top3-acc=0.998\n",
      "[Epoch 15 Batch 150/2581] loss=0.9741, lr=0.0000165217, Top3-acc=0.999\n",
      "[Epoch 15 Batch 200/2581] loss=1.1004, lr=0.0000164701, Top3-acc=0.999\n",
      "[Epoch 15 Batch 250/2581] loss=0.8600, lr=0.0000164141, Top3-acc=0.999\n",
      "[Epoch 15 Batch 300/2581] loss=0.9523, lr=0.0000163625, Top3-acc=0.999\n",
      "[Epoch 15 Batch 350/2581] loss=0.8358, lr=0.0000163065, Top3-acc=0.999\n",
      "[Epoch 15 Batch 400/2581] loss=0.9777, lr=0.0000162548, Top3-acc=0.999\n",
      "[Epoch 15 Batch 450/2581] loss=0.7983, lr=0.0000161989, Top3-acc=0.999\n",
      "[Epoch 15 Batch 500/2581] loss=0.8706, lr=0.0000161472, Top3-acc=0.999\n",
      "[Epoch 15 Batch 550/2581] loss=0.9010, lr=0.0000160913, Top3-acc=0.999\n",
      "[Epoch 15 Batch 600/2581] loss=0.9571, lr=0.0000160396, Top3-acc=0.999\n",
      "[Epoch 15 Batch 650/2581] loss=1.2373, lr=0.0000159836, Top3-acc=0.999\n",
      "[Epoch 15 Batch 700/2581] loss=1.2200, lr=0.0000159320, Top3-acc=0.999\n",
      "[Epoch 15 Batch 750/2581] loss=0.8596, lr=0.0000158760, Top3-acc=0.999\n",
      "[Epoch 15 Batch 800/2581] loss=1.1249, lr=0.0000158244, Top3-acc=0.999\n",
      "[Epoch 15 Batch 850/2581] loss=1.0878, lr=0.0000157684, Top3-acc=0.999\n",
      "[Epoch 15 Batch 900/2581] loss=0.6952, lr=0.0000157167, Top3-acc=0.999\n",
      "[Epoch 15 Batch 950/2581] loss=0.9003, lr=0.0000156608, Top3-acc=0.999\n",
      "[Epoch 15 Batch 1000/2581] loss=0.8066, lr=0.0000156091, Top3-acc=0.999\n",
      "[Epoch 15 Batch 1050/2581] loss=0.6552, lr=0.0000155532, Top3-acc=0.999\n",
      "[Epoch 15 Batch 1100/2581] loss=0.8377, lr=0.0000155015, Top3-acc=0.999\n",
      "[Epoch 15 Batch 1150/2581] loss=0.9437, lr=0.0000154455, Top3-acc=0.999\n",
      "[Epoch 15 Batch 1200/2581] loss=1.1331, lr=0.0000153939, Top3-acc=0.999\n",
      "[Epoch 15 Batch 1250/2581] loss=0.8184, lr=0.0000153379, Top3-acc=0.999\n",
      "[Epoch 15 Batch 1300/2581] loss=1.0050, lr=0.0000152863, Top3-acc=0.999\n",
      "[Epoch 15 Batch 1350/2581] loss=0.9173, lr=0.0000152303, Top3-acc=0.999\n",
      "[Epoch 15 Batch 1400/2581] loss=0.7286, lr=0.0000151786, Top3-acc=0.999\n",
      "[Epoch 15 Batch 1450/2581] loss=1.0838, lr=0.0000151227, Top3-acc=0.999\n",
      "[Epoch 15 Batch 1500/2581] loss=0.7483, lr=0.0000150710, Top3-acc=0.999\n",
      "[Epoch 15 Batch 1550/2581] loss=1.2064, lr=0.0000150151, Top3-acc=0.999\n",
      "[Epoch 15 Batch 1600/2581] loss=1.0274, lr=0.0000149634, Top3-acc=0.999\n",
      "[Epoch 15 Batch 1650/2581] loss=1.1436, lr=0.0000149074, Top3-acc=0.999\n",
      "[Epoch 15 Batch 1700/2581] loss=0.8889, lr=0.0000148558, Top3-acc=0.999\n",
      "[Epoch 15 Batch 1750/2581] loss=0.5949, lr=0.0000147998, Top3-acc=0.999\n",
      "[Epoch 15 Batch 1800/2581] loss=0.5723, lr=0.0000147482, Top3-acc=0.999\n",
      "[Epoch 15 Batch 1850/2581] loss=0.6955, lr=0.0000146922, Top3-acc=0.999\n",
      "[Epoch 15 Batch 1900/2581] loss=0.7600, lr=0.0000146406, Top3-acc=0.999\n",
      "[Epoch 15 Batch 1950/2581] loss=0.9303, lr=0.0000145846, Top3-acc=0.999\n",
      "[Epoch 15 Batch 2000/2581] loss=1.2447, lr=0.0000145329, Top3-acc=0.999\n",
      "[Epoch 15 Batch 2050/2581] loss=1.1232, lr=0.0000144770, Top3-acc=0.999\n",
      "[Epoch 15 Batch 2100/2581] loss=0.8735, lr=0.0000144253, Top3-acc=0.999\n",
      "[Epoch 15 Batch 2150/2581] loss=1.1835, lr=0.0000143693, Top3-acc=0.999\n",
      "[Epoch 15 Batch 2200/2581] loss=0.7820, lr=0.0000143177, Top3-acc=0.999\n",
      "[Epoch 15 Batch 2250/2581] loss=0.7671, lr=0.0000142617, Top3-acc=0.999\n",
      "[Epoch 15 Batch 2300/2581] loss=0.7186, lr=0.0000142101, Top3-acc=0.999\n",
      "[Epoch 15 Batch 2350/2581] loss=0.8989, lr=0.0000141541, Top3-acc=0.999\n",
      "[Epoch 15 Batch 2400/2581] loss=0.8181, lr=0.0000141025, Top3-acc=0.999\n",
      "[Epoch 15 Batch 2450/2581] loss=0.5945, lr=0.0000140465, Top3-acc=0.999\n",
      "[Epoch 15 Batch 2500/2581] loss=1.0896, lr=0.0000139948, Top3-acc=0.999\n",
      "[Epoch 15 Batch 2550/2581] loss=0.7729, lr=0.0000139389, Top3-acc=0.999\n",
      "validation Top3-Acc : 0.9048708728342596\n",
      "[Epoch 16 Batch 50/2581] loss=0.7230, lr=0.0000138528, Top3-acc=0.998\n",
      "[Epoch 16 Batch 100/2581] loss=0.9326, lr=0.0000138011, Top3-acc=0.998\n",
      "[Epoch 16 Batch 150/2581] loss=0.9335, lr=0.0000137452, Top3-acc=0.998\n",
      "[Epoch 16 Batch 200/2581] loss=1.0708, lr=0.0000136935, Top3-acc=0.999\n",
      "[Epoch 16 Batch 250/2581] loss=0.6820, lr=0.0000136375, Top3-acc=0.999\n",
      "[Epoch 16 Batch 300/2581] loss=0.9398, lr=0.0000135859, Top3-acc=0.998\n",
      "[Epoch 16 Batch 350/2581] loss=0.7131, lr=0.0000135299, Top3-acc=0.998\n",
      "[Epoch 16 Batch 400/2581] loss=0.8363, lr=0.0000134783, Top3-acc=0.999\n",
      "[Epoch 16 Batch 450/2581] loss=0.4986, lr=0.0000134223, Top3-acc=0.999\n",
      "[Epoch 16 Batch 500/2581] loss=0.8229, lr=0.0000133706, Top3-acc=0.999\n",
      "[Epoch 16 Batch 550/2581] loss=0.8366, lr=0.0000133147, Top3-acc=0.999\n",
      "[Epoch 16 Batch 600/2581] loss=0.7152, lr=0.0000132630, Top3-acc=0.999\n",
      "[Epoch 16 Batch 650/2581] loss=1.0037, lr=0.0000132071, Top3-acc=0.999\n",
      "[Epoch 16 Batch 700/2581] loss=0.9375, lr=0.0000131554, Top3-acc=0.999\n",
      "[Epoch 16 Batch 750/2581] loss=0.5900, lr=0.0000130994, Top3-acc=0.999\n",
      "[Epoch 16 Batch 800/2581] loss=0.8304, lr=0.0000130478, Top3-acc=0.999\n",
      "[Epoch 16 Batch 850/2581] loss=0.8477, lr=0.0000129918, Top3-acc=0.999\n",
      "[Epoch 16 Batch 900/2581] loss=0.6073, lr=0.0000129402, Top3-acc=0.999\n",
      "[Epoch 16 Batch 950/2581] loss=0.6598, lr=0.0000128842, Top3-acc=0.999\n",
      "[Epoch 16 Batch 1000/2581] loss=0.6237, lr=0.0000128325, Top3-acc=0.999\n",
      "[Epoch 16 Batch 1050/2581] loss=0.4020, lr=0.0000127766, Top3-acc=0.999\n",
      "[Epoch 16 Batch 1100/2581] loss=0.6846, lr=0.0000127249, Top3-acc=0.999\n",
      "[Epoch 16 Batch 1150/2581] loss=0.6670, lr=0.0000126690, Top3-acc=0.999\n",
      "[Epoch 16 Batch 1200/2581] loss=0.8916, lr=0.0000126173, Top3-acc=0.999\n",
      "[Epoch 16 Batch 1250/2581] loss=0.9011, lr=0.0000125613, Top3-acc=0.999\n",
      "[Epoch 16 Batch 1300/2581] loss=1.0384, lr=0.0000125097, Top3-acc=0.999\n",
      "[Epoch 16 Batch 1350/2581] loss=1.0642, lr=0.0000124537, Top3-acc=0.999\n",
      "[Epoch 16 Batch 1400/2581] loss=0.5793, lr=0.0000124021, Top3-acc=0.999\n",
      "[Epoch 16 Batch 1450/2581] loss=0.7635, lr=0.0000123461, Top3-acc=0.999\n",
      "[Epoch 16 Batch 1500/2581] loss=0.6928, lr=0.0000122944, Top3-acc=0.999\n",
      "[Epoch 16 Batch 1550/2581] loss=1.0319, lr=0.0000122385, Top3-acc=0.999\n",
      "[Epoch 16 Batch 1600/2581] loss=0.9693, lr=0.0000121868, Top3-acc=0.999\n",
      "[Epoch 16 Batch 1650/2581] loss=1.1908, lr=0.0000121309, Top3-acc=0.999\n",
      "[Epoch 16 Batch 1700/2581] loss=0.9795, lr=0.0000120792, Top3-acc=0.999\n",
      "[Epoch 16 Batch 1750/2581] loss=0.7200, lr=0.0000120232, Top3-acc=0.999\n",
      "[Epoch 16 Batch 1800/2581] loss=0.3835, lr=0.0000119716, Top3-acc=0.999\n",
      "[Epoch 16 Batch 1850/2581] loss=0.5961, lr=0.0000119156, Top3-acc=0.999\n",
      "[Epoch 16 Batch 1900/2581] loss=0.6501, lr=0.0000118640, Top3-acc=0.999\n",
      "[Epoch 16 Batch 1950/2581] loss=0.6502, lr=0.0000118080, Top3-acc=0.999\n",
      "[Epoch 16 Batch 2000/2581] loss=0.7758, lr=0.0000117563, Top3-acc=0.999\n",
      "[Epoch 16 Batch 2050/2581] loss=0.8720, lr=0.0000117004, Top3-acc=0.999\n",
      "[Epoch 16 Batch 2100/2581] loss=0.7917, lr=0.0000116487, Top3-acc=0.999\n",
      "[Epoch 16 Batch 2150/2581] loss=1.0388, lr=0.0000115928, Top3-acc=0.999\n",
      "[Epoch 16 Batch 2200/2581] loss=0.7722, lr=0.0000115411, Top3-acc=0.999\n",
      "[Epoch 16 Batch 2250/2581] loss=0.5935, lr=0.0000114851, Top3-acc=0.999\n",
      "[Epoch 16 Batch 2300/2581] loss=0.8303, lr=0.0000114335, Top3-acc=0.999\n",
      "[Epoch 16 Batch 2350/2581] loss=0.6275, lr=0.0000113775, Top3-acc=0.999\n",
      "[Epoch 16 Batch 2400/2581] loss=0.7579, lr=0.0000113259, Top3-acc=0.999\n",
      "[Epoch 16 Batch 2450/2581] loss=0.5193, lr=0.0000112699, Top3-acc=0.999\n",
      "[Epoch 16 Batch 2500/2581] loss=1.1638, lr=0.0000112183, Top3-acc=0.999\n",
      "[Epoch 16 Batch 2550/2581] loss=0.7274, lr=0.0000111623, Top3-acc=0.999\n",
      "validation Top3-Acc : 0.9080309469325487\n",
      "[Epoch 17 Batch 50/2581] loss=0.7177, lr=0.0000110762, Top3-acc=0.998\n",
      "[Epoch 17 Batch 100/2581] loss=0.7159, lr=0.0000110245, Top3-acc=0.998\n",
      "[Epoch 17 Batch 150/2581] loss=0.7746, lr=0.0000109686, Top3-acc=0.999\n",
      "[Epoch 17 Batch 200/2581] loss=0.7172, lr=0.0000109169, Top3-acc=0.999\n",
      "[Epoch 17 Batch 250/2581] loss=0.6835, lr=0.0000108610, Top3-acc=0.999\n",
      "[Epoch 17 Batch 300/2581] loss=0.8347, lr=0.0000108093, Top3-acc=0.999\n",
      "[Epoch 17 Batch 350/2581] loss=0.6781, lr=0.0000107533, Top3-acc=0.999\n",
      "[Epoch 17 Batch 400/2581] loss=0.7746, lr=0.0000107017, Top3-acc=0.999\n",
      "[Epoch 17 Batch 450/2581] loss=0.6542, lr=0.0000106457, Top3-acc=0.999\n",
      "[Epoch 17 Batch 500/2581] loss=0.8180, lr=0.0000105941, Top3-acc=0.999\n",
      "[Epoch 17 Batch 550/2581] loss=0.8887, lr=0.0000105381, Top3-acc=0.999\n",
      "[Epoch 17 Batch 600/2581] loss=0.5454, lr=0.0000104864, Top3-acc=0.999\n",
      "[Epoch 17 Batch 650/2581] loss=0.8882, lr=0.0000104305, Top3-acc=0.999\n",
      "[Epoch 17 Batch 700/2581] loss=0.8355, lr=0.0000103788, Top3-acc=0.999\n",
      "[Epoch 17 Batch 750/2581] loss=0.4457, lr=0.0000103229, Top3-acc=0.999\n",
      "[Epoch 17 Batch 800/2581] loss=0.7674, lr=0.0000102712, Top3-acc=0.999\n",
      "[Epoch 17 Batch 850/2581] loss=0.6020, lr=0.0000102152, Top3-acc=0.999\n",
      "[Epoch 17 Batch 900/2581] loss=0.4588, lr=0.0000101636, Top3-acc=0.999\n",
      "[Epoch 17 Batch 950/2581] loss=0.6272, lr=0.0000101076, Top3-acc=0.999\n",
      "[Epoch 17 Batch 1000/2581] loss=0.5733, lr=0.0000100560, Top3-acc=0.999\n",
      "[Epoch 17 Batch 1050/2581] loss=0.5261, lr=0.0000100000, Top3-acc=0.999\n",
      "[Epoch 17 Batch 1100/2581] loss=0.6158, lr=0.0000099483, Top3-acc=0.999\n",
      "[Epoch 17 Batch 1150/2581] loss=0.7766, lr=0.0000098924, Top3-acc=0.999\n",
      "[Epoch 17 Batch 1200/2581] loss=0.8885, lr=0.0000098407, Top3-acc=0.999\n",
      "[Epoch 17 Batch 1250/2581] loss=0.7543, lr=0.0000097848, Top3-acc=0.999\n",
      "[Epoch 17 Batch 1300/2581] loss=0.9414, lr=0.0000097331, Top3-acc=0.999\n",
      "[Epoch 17 Batch 1350/2581] loss=0.8767, lr=0.0000096771, Top3-acc=0.999\n",
      "[Epoch 17 Batch 1400/2581] loss=0.4953, lr=0.0000096255, Top3-acc=0.999\n",
      "[Epoch 17 Batch 1450/2581] loss=0.6106, lr=0.0000095695, Top3-acc=0.999\n",
      "[Epoch 17 Batch 1500/2581] loss=0.5371, lr=0.0000095179, Top3-acc=0.999\n",
      "[Epoch 17 Batch 1550/2581] loss=0.8812, lr=0.0000094619, Top3-acc=0.999\n",
      "[Epoch 17 Batch 1600/2581] loss=0.6498, lr=0.0000094102, Top3-acc=0.999\n",
      "[Epoch 17 Batch 1650/2581] loss=1.0217, lr=0.0000093543, Top3-acc=0.999\n",
      "[Epoch 17 Batch 1700/2581] loss=0.8722, lr=0.0000093026, Top3-acc=0.999\n",
      "[Epoch 17 Batch 1750/2581] loss=0.5278, lr=0.0000092467, Top3-acc=0.999\n",
      "[Epoch 17 Batch 1800/2581] loss=0.4739, lr=0.0000091950, Top3-acc=0.999\n",
      "[Epoch 17 Batch 1850/2581] loss=0.4650, lr=0.0000091390, Top3-acc=0.999\n",
      "[Epoch 17 Batch 1900/2581] loss=0.4717, lr=0.0000090874, Top3-acc=0.999\n",
      "[Epoch 17 Batch 1950/2581] loss=0.6369, lr=0.0000090314, Top3-acc=0.999\n",
      "[Epoch 17 Batch 2000/2581] loss=0.6883, lr=0.0000089798, Top3-acc=0.999\n",
      "[Epoch 17 Batch 2050/2581] loss=0.6313, lr=0.0000089238, Top3-acc=0.999\n",
      "[Epoch 17 Batch 2100/2581] loss=0.5775, lr=0.0000088721, Top3-acc=0.999\n",
      "[Epoch 17 Batch 2150/2581] loss=0.8888, lr=0.0000088162, Top3-acc=0.999\n",
      "[Epoch 17 Batch 2200/2581] loss=0.5246, lr=0.0000087645, Top3-acc=0.999\n",
      "[Epoch 17 Batch 2250/2581] loss=0.6820, lr=0.0000087086, Top3-acc=0.999\n",
      "[Epoch 17 Batch 2300/2581] loss=0.4952, lr=0.0000086569, Top3-acc=0.999\n",
      "[Epoch 17 Batch 2350/2581] loss=0.5852, lr=0.0000086009, Top3-acc=0.999\n",
      "[Epoch 17 Batch 2400/2581] loss=0.6189, lr=0.0000085493, Top3-acc=0.999\n",
      "[Epoch 17 Batch 2450/2581] loss=0.3163, lr=0.0000084933, Top3-acc=0.999\n",
      "[Epoch 17 Batch 2500/2581] loss=1.0325, lr=0.0000084417, Top3-acc=0.999\n",
      "[Epoch 17 Batch 2550/2581] loss=0.6742, lr=0.0000083857, Top3-acc=0.999\n",
      "validation Top3-Acc : 0.9103192764519996\n",
      "[Epoch 18 Batch 50/2581] loss=0.5500, lr=0.0000082996, Top3-acc=0.999\n",
      "[Epoch 18 Batch 100/2581] loss=0.5997, lr=0.0000082480, Top3-acc=0.999\n",
      "[Epoch 18 Batch 150/2581] loss=0.8405, lr=0.0000081920, Top3-acc=0.999\n",
      "[Epoch 18 Batch 200/2581] loss=0.5348, lr=0.0000081403, Top3-acc=0.999\n",
      "[Epoch 18 Batch 250/2581] loss=0.4966, lr=0.0000080844, Top3-acc=1.000\n",
      "[Epoch 18 Batch 300/2581] loss=0.6369, lr=0.0000080327, Top3-acc=0.999\n",
      "[Epoch 18 Batch 350/2581] loss=0.4665, lr=0.0000079768, Top3-acc=0.999\n",
      "[Epoch 18 Batch 400/2581] loss=0.5594, lr=0.0000079251, Top3-acc=1.000\n",
      "[Epoch 18 Batch 450/2581] loss=0.4476, lr=0.0000078691, Top3-acc=1.000\n",
      "[Epoch 18 Batch 500/2581] loss=0.5904, lr=0.0000078175, Top3-acc=1.000\n",
      "[Epoch 18 Batch 550/2581] loss=0.7438, lr=0.0000077615, Top3-acc=1.000\n",
      "[Epoch 18 Batch 600/2581] loss=0.4561, lr=0.0000077099, Top3-acc=1.000\n",
      "[Epoch 18 Batch 650/2581] loss=0.4724, lr=0.0000076539, Top3-acc=1.000\n",
      "[Epoch 18 Batch 700/2581] loss=0.6913, lr=0.0000076022, Top3-acc=1.000\n",
      "[Epoch 18 Batch 750/2581] loss=0.3660, lr=0.0000075463, Top3-acc=1.000\n",
      "[Epoch 18 Batch 800/2581] loss=0.6185, lr=0.0000074946, Top3-acc=1.000\n",
      "[Epoch 18 Batch 850/2581] loss=0.7208, lr=0.0000074387, Top3-acc=1.000\n",
      "[Epoch 18 Batch 900/2581] loss=0.3506, lr=0.0000073870, Top3-acc=1.000\n",
      "[Epoch 18 Batch 950/2581] loss=0.5106, lr=0.0000073310, Top3-acc=1.000\n",
      "[Epoch 18 Batch 1000/2581] loss=0.3962, lr=0.0000072794, Top3-acc=1.000\n",
      "[Epoch 18 Batch 1050/2581] loss=0.3397, lr=0.0000072234, Top3-acc=1.000\n",
      "[Epoch 18 Batch 1100/2581] loss=0.4418, lr=0.0000071718, Top3-acc=1.000\n",
      "[Epoch 18 Batch 1150/2581] loss=0.5511, lr=0.0000071158, Top3-acc=1.000\n",
      "[Epoch 18 Batch 1200/2581] loss=0.6408, lr=0.0000070641, Top3-acc=1.000\n",
      "[Epoch 18 Batch 1250/2581] loss=0.7018, lr=0.0000070082, Top3-acc=1.000\n",
      "[Epoch 18 Batch 1300/2581] loss=0.7124, lr=0.0000069565, Top3-acc=1.000\n",
      "[Epoch 18 Batch 1350/2581] loss=0.7320, lr=0.0000069006, Top3-acc=0.999\n",
      "[Epoch 18 Batch 1400/2581] loss=0.6525, lr=0.0000068489, Top3-acc=0.999\n",
      "[Epoch 18 Batch 1450/2581] loss=0.5758, lr=0.0000067929, Top3-acc=0.999\n",
      "[Epoch 18 Batch 1500/2581] loss=0.5823, lr=0.0000067413, Top3-acc=0.999\n",
      "[Epoch 18 Batch 1550/2581] loss=0.5612, lr=0.0000066853, Top3-acc=0.999\n",
      "[Epoch 18 Batch 1600/2581] loss=0.4930, lr=0.0000066337, Top3-acc=0.999\n",
      "[Epoch 18 Batch 1650/2581] loss=1.0045, lr=0.0000065777, Top3-acc=0.999\n",
      "[Epoch 18 Batch 1700/2581] loss=0.6583, lr=0.0000065260, Top3-acc=0.999\n",
      "[Epoch 18 Batch 1750/2581] loss=0.3636, lr=0.0000064701, Top3-acc=0.999\n",
      "[Epoch 18 Batch 1800/2581] loss=0.4418, lr=0.0000064184, Top3-acc=0.999\n",
      "[Epoch 18 Batch 1850/2581] loss=0.4416, lr=0.0000063625, Top3-acc=0.999\n",
      "[Epoch 18 Batch 1900/2581] loss=0.4555, lr=0.0000063108, Top3-acc=0.999\n",
      "[Epoch 18 Batch 1950/2581] loss=0.4611, lr=0.0000062548, Top3-acc=0.999\n",
      "[Epoch 18 Batch 2000/2581] loss=0.5830, lr=0.0000062032, Top3-acc=0.999\n",
      "[Epoch 18 Batch 2050/2581] loss=0.6581, lr=0.0000061472, Top3-acc=0.999\n",
      "[Epoch 18 Batch 2100/2581] loss=0.6509, lr=0.0000060956, Top3-acc=0.999\n",
      "[Epoch 18 Batch 2150/2581] loss=0.7375, lr=0.0000060396, Top3-acc=0.999\n",
      "[Epoch 18 Batch 2200/2581] loss=0.4965, lr=0.0000059879, Top3-acc=0.999\n",
      "[Epoch 18 Batch 2250/2581] loss=0.5879, lr=0.0000059320, Top3-acc=0.999\n",
      "[Epoch 18 Batch 2300/2581] loss=0.3582, lr=0.0000058803, Top3-acc=0.999\n",
      "[Epoch 18 Batch 2350/2581] loss=0.3871, lr=0.0000058244, Top3-acc=0.999\n",
      "[Epoch 18 Batch 2400/2581] loss=0.6394, lr=0.0000057727, Top3-acc=0.999\n",
      "[Epoch 18 Batch 2450/2581] loss=0.4338, lr=0.0000057167, Top3-acc=0.999\n",
      "[Epoch 18 Batch 2500/2581] loss=0.7540, lr=0.0000056651, Top3-acc=0.999\n",
      "[Epoch 18 Batch 2550/2581] loss=0.4968, lr=0.0000056091, Top3-acc=0.999\n",
      "validation Top3-Acc : 0.9105372125967092\n",
      "[Epoch 19 Batch 50/2581] loss=0.4880, lr=0.0000055230, Top3-acc=0.999\n",
      "[Epoch 19 Batch 100/2581] loss=0.5794, lr=0.0000054714, Top3-acc=0.999\n",
      "[Epoch 19 Batch 150/2581] loss=0.5894, lr=0.0000054154, Top3-acc=0.999\n",
      "[Epoch 19 Batch 200/2581] loss=0.6731, lr=0.0000053638, Top3-acc=0.999\n",
      "[Epoch 19 Batch 250/2581] loss=0.4907, lr=0.0000053078, Top3-acc=1.000\n",
      "[Epoch 19 Batch 300/2581] loss=0.4526, lr=0.0000052561, Top3-acc=0.999\n",
      "[Epoch 19 Batch 350/2581] loss=0.4689, lr=0.0000052002, Top3-acc=0.999\n",
      "[Epoch 19 Batch 400/2581] loss=0.5643, lr=0.0000051485, Top3-acc=1.000\n",
      "[Epoch 19 Batch 450/2581] loss=0.4267, lr=0.0000050926, Top3-acc=1.000\n",
      "[Epoch 19 Batch 500/2581] loss=0.4345, lr=0.0000050409, Top3-acc=1.000\n",
      "[Epoch 19 Batch 550/2581] loss=0.4725, lr=0.0000049849, Top3-acc=1.000\n",
      "[Epoch 19 Batch 600/2581] loss=0.3021, lr=0.0000049333, Top3-acc=1.000\n",
      "[Epoch 19 Batch 650/2581] loss=0.3964, lr=0.0000048773, Top3-acc=1.000\n",
      "[Epoch 19 Batch 700/2581] loss=0.6265, lr=0.0000048257, Top3-acc=1.000\n",
      "[Epoch 19 Batch 750/2581] loss=0.4287, lr=0.0000047697, Top3-acc=1.000\n",
      "[Epoch 19 Batch 800/2581] loss=0.4400, lr=0.0000047180, Top3-acc=1.000\n",
      "[Epoch 19 Batch 850/2581] loss=0.7354, lr=0.0000046621, Top3-acc=1.000\n",
      "[Epoch 19 Batch 900/2581] loss=0.2952, lr=0.0000046104, Top3-acc=1.000\n",
      "[Epoch 19 Batch 950/2581] loss=0.5161, lr=0.0000045545, Top3-acc=1.000\n",
      "[Epoch 19 Batch 1000/2581] loss=0.2580, lr=0.0000045028, Top3-acc=1.000\n",
      "[Epoch 19 Batch 1050/2581] loss=0.2914, lr=0.0000044468, Top3-acc=1.000\n",
      "[Epoch 19 Batch 1100/2581] loss=0.3624, lr=0.0000043952, Top3-acc=1.000\n",
      "[Epoch 19 Batch 1150/2581] loss=0.4200, lr=0.0000043392, Top3-acc=1.000\n",
      "[Epoch 19 Batch 1200/2581] loss=0.5967, lr=0.0000042876, Top3-acc=1.000\n",
      "[Epoch 19 Batch 1250/2581] loss=0.6230, lr=0.0000042316, Top3-acc=1.000\n",
      "[Epoch 19 Batch 1300/2581] loss=0.6682, lr=0.0000041799, Top3-acc=1.000\n",
      "[Epoch 19 Batch 1350/2581] loss=0.6506, lr=0.0000041240, Top3-acc=1.000\n",
      "[Epoch 19 Batch 1400/2581] loss=0.4754, lr=0.0000040723, Top3-acc=1.000\n",
      "[Epoch 19 Batch 1450/2581] loss=0.6643, lr=0.0000040164, Top3-acc=1.000\n",
      "[Epoch 19 Batch 1500/2581] loss=0.6423, lr=0.0000039647, Top3-acc=1.000\n",
      "[Epoch 19 Batch 1550/2581] loss=0.6644, lr=0.0000039087, Top3-acc=0.999\n",
      "[Epoch 19 Batch 1600/2581] loss=0.5561, lr=0.0000038571, Top3-acc=1.000\n",
      "[Epoch 19 Batch 1650/2581] loss=0.7059, lr=0.0000038011, Top3-acc=0.999\n",
      "[Epoch 19 Batch 1700/2581] loss=0.5377, lr=0.0000037495, Top3-acc=0.999\n",
      "[Epoch 19 Batch 1750/2581] loss=0.2539, lr=0.0000036935, Top3-acc=0.999\n",
      "[Epoch 19 Batch 1800/2581] loss=0.4548, lr=0.0000036418, Top3-acc=0.999\n",
      "[Epoch 19 Batch 1850/2581] loss=0.3237, lr=0.0000035859, Top3-acc=0.999\n",
      "[Epoch 19 Batch 1900/2581] loss=0.3949, lr=0.0000035342, Top3-acc=1.000\n",
      "[Epoch 19 Batch 1950/2581] loss=0.5874, lr=0.0000034783, Top3-acc=1.000\n",
      "[Epoch 19 Batch 2000/2581] loss=0.7253, lr=0.0000034266, Top3-acc=0.999\n",
      "[Epoch 19 Batch 2050/2581] loss=0.5837, lr=0.0000033706, Top3-acc=0.999\n",
      "[Epoch 19 Batch 2100/2581] loss=0.4325, lr=0.0000033190, Top3-acc=0.999\n",
      "[Epoch 19 Batch 2150/2581] loss=0.7287, lr=0.0000032630, Top3-acc=0.999\n",
      "[Epoch 19 Batch 2200/2581] loss=0.3959, lr=0.0000032114, Top3-acc=0.999\n",
      "[Epoch 19 Batch 2250/2581] loss=0.3857, lr=0.0000031554, Top3-acc=0.999\n",
      "[Epoch 19 Batch 2300/2581] loss=0.3404, lr=0.0000031037, Top3-acc=0.999\n",
      "[Epoch 19 Batch 2350/2581] loss=0.5045, lr=0.0000030478, Top3-acc=0.999\n",
      "[Epoch 19 Batch 2400/2581] loss=0.5391, lr=0.0000029961, Top3-acc=0.999\n",
      "[Epoch 19 Batch 2450/2581] loss=0.2925, lr=0.0000029402, Top3-acc=0.999\n",
      "[Epoch 19 Batch 2500/2581] loss=0.7435, lr=0.0000028885, Top3-acc=0.999\n",
      "[Epoch 19 Batch 2550/2581] loss=0.4275, lr=0.0000028325, Top3-acc=0.999\n",
      "validation Top3-Acc : 0.9122807017543859\n",
      "[Epoch 20 Batch 50/2581] loss=0.2772, lr=0.0000027464, Top3-acc=0.999\n",
      "[Epoch 20 Batch 100/2581] loss=0.5120, lr=0.0000026948, Top3-acc=0.999\n",
      "[Epoch 20 Batch 150/2581] loss=0.4157, lr=0.0000026388, Top3-acc=0.999\n",
      "[Epoch 20 Batch 200/2581] loss=0.5743, lr=0.0000025872, Top3-acc=1.000\n",
      "[Epoch 20 Batch 250/2581] loss=0.4988, lr=0.0000025312, Top3-acc=1.000\n",
      "[Epoch 20 Batch 300/2581] loss=0.5097, lr=0.0000024796, Top3-acc=1.000\n",
      "[Epoch 20 Batch 350/2581] loss=0.4383, lr=0.0000024236, Top3-acc=1.000\n",
      "[Epoch 20 Batch 400/2581] loss=0.5008, lr=0.0000023719, Top3-acc=1.000\n",
      "[Epoch 20 Batch 450/2581] loss=0.4646, lr=0.0000023160, Top3-acc=1.000\n",
      "[Epoch 20 Batch 500/2581] loss=0.2872, lr=0.0000022643, Top3-acc=1.000\n",
      "[Epoch 20 Batch 550/2581] loss=0.3696, lr=0.0000022084, Top3-acc=1.000\n",
      "[Epoch 20 Batch 600/2581] loss=0.2859, lr=0.0000021567, Top3-acc=1.000\n",
      "[Epoch 20 Batch 650/2581] loss=0.4130, lr=0.0000021007, Top3-acc=1.000\n",
      "[Epoch 20 Batch 700/2581] loss=0.3650, lr=0.0000020491, Top3-acc=1.000\n",
      "[Epoch 20 Batch 750/2581] loss=0.2583, lr=0.0000019931, Top3-acc=1.000\n",
      "[Epoch 20 Batch 800/2581] loss=0.4582, lr=0.0000019415, Top3-acc=1.000\n",
      "[Epoch 20 Batch 850/2581] loss=0.6498, lr=0.0000018855, Top3-acc=1.000\n",
      "[Epoch 20 Batch 900/2581] loss=0.2727, lr=0.0000018338, Top3-acc=1.000\n",
      "[Epoch 20 Batch 950/2581] loss=0.4351, lr=0.0000017779, Top3-acc=1.000\n",
      "[Epoch 20 Batch 1000/2581] loss=0.2481, lr=0.0000017262, Top3-acc=1.000\n",
      "[Epoch 20 Batch 1050/2581] loss=0.3062, lr=0.0000016703, Top3-acc=1.000\n",
      "[Epoch 20 Batch 1100/2581] loss=0.3377, lr=0.0000016186, Top3-acc=1.000\n",
      "[Epoch 20 Batch 1150/2581] loss=0.3676, lr=0.0000015626, Top3-acc=1.000\n",
      "[Epoch 20 Batch 1200/2581] loss=0.3908, lr=0.0000015110, Top3-acc=1.000\n",
      "[Epoch 20 Batch 1250/2581] loss=0.4181, lr=0.0000014550, Top3-acc=1.000\n",
      "[Epoch 20 Batch 1300/2581] loss=0.4324, lr=0.0000014034, Top3-acc=1.000\n",
      "[Epoch 20 Batch 1350/2581] loss=0.5747, lr=0.0000013474, Top3-acc=1.000\n",
      "[Epoch 20 Batch 1400/2581] loss=0.4678, lr=0.0000012957, Top3-acc=1.000\n",
      "[Epoch 20 Batch 1450/2581] loss=0.5021, lr=0.0000012398, Top3-acc=1.000\n",
      "[Epoch 20 Batch 1500/2581] loss=0.5954, lr=0.0000011881, Top3-acc=1.000\n",
      "[Epoch 20 Batch 1550/2581] loss=0.5455, lr=0.0000011322, Top3-acc=1.000\n",
      "[Epoch 20 Batch 1600/2581] loss=0.5190, lr=0.0000010805, Top3-acc=1.000\n",
      "[Epoch 20 Batch 1650/2581] loss=0.7011, lr=0.0000010245, Top3-acc=1.000\n",
      "[Epoch 20 Batch 1700/2581] loss=0.4933, lr=0.0000009729, Top3-acc=1.000\n",
      "[Epoch 20 Batch 1750/2581] loss=0.3024, lr=0.0000009169, Top3-acc=1.000\n",
      "[Epoch 20 Batch 1800/2581] loss=0.3290, lr=0.0000008653, Top3-acc=1.000\n",
      "[Epoch 20 Batch 1850/2581] loss=0.3023, lr=0.0000008093, Top3-acc=1.000\n",
      "[Epoch 20 Batch 1900/2581] loss=0.3373, lr=0.0000007576, Top3-acc=1.000\n",
      "[Epoch 20 Batch 1950/2581] loss=0.5172, lr=0.0000007017, Top3-acc=1.000\n",
      "[Epoch 20 Batch 2000/2581] loss=0.6683, lr=0.0000006500, Top3-acc=1.000\n",
      "[Epoch 20 Batch 2050/2581] loss=0.4363, lr=0.0000005941, Top3-acc=1.000\n",
      "[Epoch 20 Batch 2100/2581] loss=0.4934, lr=0.0000005424, Top3-acc=1.000\n",
      "[Epoch 20 Batch 2150/2581] loss=0.6997, lr=0.0000004864, Top3-acc=0.999\n",
      "[Epoch 20 Batch 2200/2581] loss=0.3654, lr=0.0000004348, Top3-acc=0.999\n",
      "[Epoch 20 Batch 2250/2581] loss=0.2800, lr=0.0000003788, Top3-acc=0.999\n",
      "[Epoch 20 Batch 2300/2581] loss=0.3898, lr=0.0000003272, Top3-acc=0.999\n",
      "[Epoch 20 Batch 2350/2581] loss=0.3571, lr=0.0000002712, Top3-acc=0.999\n",
      "[Epoch 20 Batch 2400/2581] loss=0.4451, lr=0.0000002195, Top3-acc=0.999\n",
      "[Epoch 20 Batch 2450/2581] loss=0.3300, lr=0.0000001636, Top3-acc=0.999\n",
      "[Epoch 20 Batch 2500/2581] loss=0.6738, lr=0.0000001119, Top3-acc=0.999\n",
      "[Epoch 20 Batch 2550/2581] loss=0.4245, lr=0.0000000560, Top3-acc=0.999\n",
      "validation Top3-Acc : 0.9116268933202571\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "for epoch_id in range(num_epochs):\n",
    "    metric.reset()\n",
    "    step_loss = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(train_dataloader):\n",
    "        if step_num < num_warmup_steps:\n",
    "            new_lr = lr * step_num / num_warmup_steps\n",
    "        else:\n",
    "            non_warmup_steps = step_num - num_warmup_steps\n",
    "            offset = non_warmup_steps / (num_train_steps - num_warmup_steps)\n",
    "            new_lr = lr - offset * lr\n",
    "        trainer.set_learning_rate(new_lr)\n",
    "        \n",
    "        with mx.autograd.record():\n",
    "            # load data to Multi-GPU\n",
    "            token_ids = gluon.utils.split_and_load(token_ids, devices)\n",
    "            valid_length = gluon.utils.split_and_load(valid_length, devices)\n",
    "            segment_ids = gluon.utils.split_and_load(segment_ids, devices)\n",
    "            label = gluon.utils.split_and_load(label, devices)\n",
    "            \n",
    "            # forward computation\n",
    "            out = [model(ti, si, vl.astype('float32')) for ti, si, vl in zip(token_ids, segment_ids, valid_length)]\n",
    "            losses = [loss_function(o,l).mean() for o, l in zip(out, label)]\n",
    "\n",
    "        # backward computation\n",
    "        for l in losses:\n",
    "            l.backward()\n",
    "    \n",
    "        if not accumulate or (batch_id + 1) % accumulate == 0:\n",
    "            trainer.allreduce_grads()\n",
    "            nlp.utils.clip_grad_global_norm(params, 1)\n",
    "            trainer.update(accumulate if accumulate else 1)\n",
    "            step_num += 1\n",
    "            if accumulate and accumulate > 1:\n",
    "                # set grad to zero for gradient accumulation\n",
    "                all_model_params.zero_grad()\n",
    "                \n",
    "        step_loss += sum([l.sum().asscalar() for l in losses])/len(devices)\n",
    "        metric.update(label, out)\n",
    "        if (batch_id + 1) % (50) == 0:\n",
    "            print(f'[Epoch {epoch_id + 1} Batch {batch_id + 1}/{len(train_dataloader)}] loss={step_loss / log_interval:.4f}, lr={trainer.learning_rate:.10f}, Top3-acc={metric.get()[1]:.3f}')\n",
    "            step_loss = 0\n",
    "    validation_acc = evaluate_accuracy(model, validation_dataloader, devices)\n",
    "    print('validation Top3-Acc : {}'.format(validation_acc))\n",
    "toc = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('data/voc-web-test.csv', sep ='\\t').loc[:,['text','class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = test_set.loc[test_set['text'].isna().apply(lambda elm: not elm), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['class'] = test_set['class'].apply(lambda x:utils_config['label2idx'][x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test.npy',test_set.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = nlp.data.NumpyDataset('test.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataloader = mx.gluon.data.DataLoader(data_test, batch_size=int(batch_size/2), num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9143865842894969"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(model, validation_dataloader, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 에폭의 학습 시간 : 7.897514385249879 시간\n"
     ]
    }
   ],
   "source": [
    "print(f'{num_epochs} 에폭의 학습 시간 : {(toc-tic)/(3600)} 시간')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
